{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 192\u001b[0m\n\u001b[1;32m    189\u001b[0m station_setting \u001b[38;5;241m=\u001b[39m station_setting_dict[configs\u001b[38;5;241m.\u001b[39muse_norm]\n\u001b[1;32m    191\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m norm \u001b[38;5;241m=\u001b[39m norm_dict[configs\u001b[38;5;241m.\u001b[39muse_norm]\u001b[38;5;241m.\u001b[39mModel(configs)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    194\u001b[0m model_optim \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:916\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:916\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from types import SimpleNamespace\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from utils.metrics import metric\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from models import DLinear, iTransformer, Autoformer, FEDformer, Linear\n",
    "from normalizers import SAN, DDN, TP, RevIN, LightTrend, NoNorm\n",
    "from layers.decomposition import series_decomp, ema_decomp, envelope_decomp\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    task_name = \"long_term_forecast\",\n",
    "    model_name = \"iTransformer\", \n",
    "    \n",
    "    # Task\n",
    "    seq_len=720,\n",
    "    label_len=168,\n",
    "    pred_len=720,\n",
    "    enc_in=7,\n",
    "    dec_in=7,\n",
    "    c_out=7,\n",
    "    features='M',\n",
    "    freq='h',\n",
    "    \n",
    "    # Data\n",
    "    root_path='datasets/ETT-small',\n",
    "    data='ETTh1',\n",
    "    data_path='ETTh1.csv',\n",
    "    target='OT',\n",
    "    batch_size=32,\n",
    "    \n",
    "    # Basic Model Settings\n",
    "    checkpoints='./manual_exp/checkpoints/',\n",
    "    dropout=0.1,\n",
    "    embed='timeF',\n",
    "    output_attention=False,\n",
    "    activation='gelu',\n",
    "    moving_avg=25,\n",
    "    num_kernels=6,\n",
    "    individual=False,\n",
    "    learning_rate=0.0001,\n",
    "    num_workers=5,\n",
    "    n_heads=8,\n",
    "    patience=3,\n",
    "    train_epochs=10,\n",
    "    use_amp=False,\n",
    "    lradj='type1',\n",
    "    embed_type=0,\n",
    "    # e_layers=4,\n",
    "    factor=3,\n",
    "    # d_model=512,\n",
    "    # d_ff=2048,\n",
    "    \n",
    "    # iTransformer\n",
    "    d_model=128,\n",
    "    d_ff=128,\n",
    "    class_strategy='projection',\n",
    "    \n",
    "    # Autoformer & FEDformer\n",
    "    # factor=3,\n",
    "    e_layers=2,\n",
    "    d_layers=1,\n",
    "    \n",
    "    # Normalizer\n",
    "    use_norm='lt',\n",
    "    decomp_type='sma',\n",
    "    norm_variant='original',\n",
    "    station_type='adaptive',\n",
    "    \n",
    "    station_joint_lr=0.0005,\n",
    "    station_pre_lr=0.00005,\n",
    "    t_ff=64,\n",
    "    alpha=0.04,\n",
    "    kernel_size=25,\n",
    "    twice_epoch=3,\n",
    "    use_mlp=False,\n",
    "    t_norm=True,\n",
    "    s_norm=True,\n",
    "    affine=True,\n",
    "    \n",
    "    period_len=24,\n",
    "    pre_epoch=5,\n",
    "    j=1,\n",
    "    learnable=False,\n",
    "    wavelet='coif3',\n",
    "    dr=0.05,\n",
    "    kernel_len=7,\n",
    "    hkernel_len=5,\n",
    "    pd_ff=128,\n",
    "    pd_model=128,\n",
    "    pe_layers=0,\n",
    "    reinit_mean_pretrain=False,\n",
    "    reinit_std_pretrain=False,\n",
    ")\n",
    "model_dict={\n",
    "    'DLinear': DLinear,\n",
    "    'iTransformer': iTransformer,\n",
    "    'Autoformer': Autoformer,\n",
    "    'FEDformer': FEDformer,\n",
    "    'Linear': Linear,\n",
    "}\n",
    "linear_models = ['DLinear', 'Linear']\n",
    "norm_dict = {\n",
    "    'none': NoNorm,\n",
    "    'revin': RevIN,\n",
    "    'san': SAN,\n",
    "    'ddn': DDN,\n",
    "    'tp': TP,\n",
    "    'lt': LightTrend,\n",
    "}\n",
    "\n",
    "if configs.decomp_type == 'sma':\n",
    "    decomp = series_decomp(configs.kernel_size)\n",
    "elif configs.decomp_type == 'ema':\n",
    "    decomp = ema_decomp(configs.alpha)\n",
    "elif configs.decomp_type == 'envelope':\n",
    "    decomp = envelope_decomp(configs.kernel_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "norm_criterion = nn.MSELoss()\n",
    "# norm_criterion = nn.L1Loss()\n",
    "\n",
    "# SAN\n",
    "def san_loss(y, statistics_pred):\n",
    "    bs, len, dim = y.shape\n",
    "    y = y.reshape(bs, -1, configs.period_len, dim)\n",
    "    mean = torch.mean(y, dim=2)\n",
    "    std = torch.std(y, dim=2)\n",
    "    station_ture = torch.cat([mean, std], dim=-1)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# DDN\n",
    "def ddn_loss(y, statistics_pred):\n",
    "    _, (mean, std) = norm.norm(y.transpose(-1, -2), False)\n",
    "    station_ture = torch.cat([mean, std], dim=1).transpose(-1, -2)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# TREAD\n",
    "def tread_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred[-1]\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "# LightTrend\n",
    "def lt_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = norm_criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "station_loss_dict = {\n",
    "    'none': None,\n",
    "    'revin': None,\n",
    "    'san': san_loss,\n",
    "    'ddn': ddn_loss,\n",
    "    'san_exp': san_loss,\n",
    "    'ddn_exp': ddn_loss,\n",
    "    'tp': tread_loss,\n",
    "    'tp2': tread_loss,\n",
    "    'lt': lt_loss,\n",
    "}\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "\n",
    "# [pre train, pre epoch, joint train, join epoch]\n",
    "station_setting_dict = {\n",
    "    'none': [0, 0, 0, 0],\n",
    "    'revin': [0, 0, 0, 0],\n",
    "    'san': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp2': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    # 'lt': [1, configs.pre_epoch, 0, 0],\n",
    "    'lt': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'san_exp': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn_exp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "}\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_pre_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion, epoch):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "\n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float()\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "            total_loss.append(loss.cpu().item())\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    norm.train()\n",
    "    return total_loss\n",
    "\n",
    "def _get_data(flag):\n",
    "    data_set, data_loader = data_provider(configs, flag)\n",
    "    return data_set, data_loader\n",
    "\n",
    "def train(setting):\n",
    "    train_data, train_loader = _get_data(flag='train')\n",
    "    vali_data, vali_loader = _get_data(flag='val')\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    path = os.path.join(configs.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    path_station = './station/' + '{}_s{}_p{}'.format(configs.use_norm, configs.data,\n",
    "                                                        configs.seq_len, configs.pred_len)\n",
    "    if not os.path.exists(path_station):\n",
    "        os.makedirs(path_station)\n",
    "\n",
    "    time_now = time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "    early_stopping_station_model = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "\n",
    "    if configs.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    ### NEW ###\n",
    "    # 재초기화를 위한 헬퍼 함수 정의\n",
    "    def reinit_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters() # PyTorch 기본값으로 가중치 초기화\n",
    "    ### END NEW ###\n",
    "\n",
    "    time_per_epoch = []\n",
    "    for epoch in range(configs.train_epochs + station_setting[1]):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        epoch_start = time()\n",
    "        # Load best station model after pretraining\n",
    "        if station_setting[0] > 0 and epoch == station_setting[1]:\n",
    "            best_model_path = path_station + '/' + 'checkpoint.pth'\n",
    "            norm.load_state_dict(torch.load(best_model_path))\n",
    "            print('loading pretrained adaptive station model')\n",
    "            \n",
    "            ### NEW ###\n",
    "            # --- Motivating Experiment 3.2: 학습 분리 효과 검증 ---\n",
    "            # 'DDN + w/o trend pretrain' 실험\n",
    "            if getattr(configs, 'reinit_mean_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing MEAN predictor weights !!!\")\n",
    "                norm.mlp.m_project.apply(reinit_weights)\n",
    "                norm.mlp.mean_proj.apply(reinit_weights)\n",
    "                norm.mlp.m_concat.apply(reinit_weights)\n",
    "                norm.mlp.mean_ffn.apply(reinit_weights)\n",
    "                norm.mlp.mean_pred.apply(reinit_weights)\n",
    "                print(\"!!! Mean predictor re-initialization complete. !!!\")\n",
    "\n",
    "            # 'DDN + w/o seasonal pretrain' (std) 실험\n",
    "            if getattr(configs, 'reinit_std_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing STD predictor weights !!!\")\n",
    "                norm.mlp.s_project.apply(reinit_weights)\n",
    "                norm.mlp.std_proj.apply(reinit_weights)\n",
    "                norm.mlp.s_concat.apply(reinit_weights)\n",
    "                norm.mlp.std_ffn.apply(reinit_weights)\n",
    "                norm.mlp.std_pred.apply(reinit_weights)\n",
    "                print(\"!!! Std predictor re-initialization complete. !!!\")\n",
    "            \n",
    "            # 'DDN + w/o pretrain' 실험은 station_setting[1] = 0 으로 설정하고\n",
    "            # 이 if 블록에 들어오지 않게 함으로써 자연스럽게 구현됩니다.\n",
    "            ### END NEW ###\n",
    "        \n",
    "        # Add station parameters to model optim after pretraining and delay epochs for joint training\n",
    "        if station_setting[2] > 0 and station_setting[3] == epoch - station_setting[1] + 1:\n",
    "            # lr = model_optim.param_groups[0]['lr']\n",
    "            lr = configs.station_joint_lr\n",
    "            # lr = norm_optim.param_groups[0]['lr']\n",
    "            model_optim.add_param_group({'params': norm.parameters(), 'lr': lr})\n",
    "        \n",
    "        model.train()\n",
    "        norm.train()\n",
    "        epoch_time = time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            \n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                        f_dim = -1 if configs.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                    f_dim = -1 if configs.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                    if configs.features == 'MS':\n",
    "                        statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                        \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time() - time_now) / iter_count\n",
    "                left_time = speed * (\n",
    "                        (configs.train_epochs + station_setting[1] - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time()\n",
    "            if configs.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                # two-stage training schema\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    norm_optim.step()\n",
    "                else:\n",
    "                    model_optim.step()\n",
    "                model_optim.zero_grad()\n",
    "                norm_optim.zero_grad()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion, epoch)\n",
    "        test_loss = vali(test_data, test_loader, criterion, epoch)\n",
    "\n",
    "        if epoch + 1 <= station_setting[1]:\n",
    "            print(\n",
    "                \"Station Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping_station_model(vali_loss, norm, path_station)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1, configs, configs.station_pre_lr)\n",
    "        else:\n",
    "            print(\n",
    "                \"Backbone Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1 - station_setting[1], train_steps, train_loss, vali_loss, test_loss))\n",
    "            # if: joint training, else: only model training\n",
    "            if station_setting[2] > 0 and station_setting[3] <= epoch - station_setting[1]:\n",
    "                early_stopping(vali_loss, model, path, norm, path_station)\n",
    "            else:\n",
    "                early_stopping(vali_loss, model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(model_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.learning_rate)\n",
    "            # adjust_learning_rate(norm_optim, epoch + 1 - station_setting[1], configs,\n",
    "            #                         configs.station_pre_lr)\n",
    "        time_per_epoch.append(time() - epoch_start)\n",
    "    print(\"Average time per pretrain epoch: {:.5f}s\".format(np.mean(time_per_epoch[:5])))\n",
    "    print(\"Average time per joint train epoch: {:.5f}s\".format(np.mean(time_per_epoch[5:])))\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if station_setting[2] > 0:\n",
    "        norm.load_state_dict(torch.load(path_station + '/' + 'checkpoint.pth'))\n",
    "    return model\n",
    "\n",
    "def check_stationarity(batch_x, test='adf'):\n",
    "    ts = batch_x[0, :, -1]\n",
    "    if test == 'adf':\n",
    "        stat, pvalue, *_ = adfuller(ts)\n",
    "        # 낮은 p-value일수록 정상성 ↑ → 점수는 1 - pvalue\n",
    "        score = 1 - min(pvalue, 1.0)\n",
    "    elif test == 'kpss':\n",
    "        stat, pvalue, *_ = kpss(ts, nlags=\"auto\")\n",
    "        # 높은 p-value일수록 정상성 ↑\n",
    "        score = min(pvalue, 1.0)\n",
    "    return score\n",
    "\n",
    "def test(setting, test=0):\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    if test:\n",
    "        print('loading model')\n",
    "        model.load_state_dict(torch.load(os.path.join('./manual_exp/checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    inputx = []\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            input_x = batch_x\n",
    "\n",
    "            # normalize\n",
    "            input_noNorm = batch_x.clone()\n",
    "            _, trend_noNorm = decomp(input_noNorm)\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            input_withNorm = batch_x\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder x\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "            dec_label = batch_x[:, -configs.label_len:, :]\n",
    "            dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "            # encoder - decoder\n",
    "            if configs.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            else:\n",
    "                if configs.model_name in linear_models:\n",
    "                    outputs = model(batch_x)\n",
    "                else:\n",
    "                    if configs.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "            f_dim = -1 if configs.features == 'MS' else 0\n",
    "            outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "            if configs.features == 'MS':\n",
    "                statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                \n",
    "            # de-normalize\n",
    "            output_noDenorm = outputs.clone()\n",
    "            outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "            output_withDenorm = outputs\n",
    "            \n",
    "            batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "            _, trend_true = decomp(batch_y)\n",
    "            trend_true = trend_true.detach().cpu().numpy()\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "            pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "            true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "            \n",
    "            # LightTrend가 예측한 추세 시각화\n",
    "            output_station_noDenorm = statistics_pred.clone() if configs.use_norm == 'lt' else None\n",
    "\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "            inputx.append(batch_x.detach().cpu().numpy())\n",
    "            if i % 20 == 0:\n",
    "                x = input_x.detach().cpu().numpy()\n",
    "                gt = np.concatenate((x[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                pd = np.concatenate((x[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "            \n",
    "            # visualize input and output with/without normalization/denormalization\n",
    "            if i == 0:\n",
    "                i_n = input_noNorm.detach().cpu().numpy()\n",
    "                t_n = trend_noNorm.detach().cpu().numpy()\n",
    "                i_w = input_withNorm.detach().cpu().numpy()\n",
    "                o_n = output_noDenorm.detach().cpu().numpy()\n",
    "                o_w = output_withDenorm.detach().cpu().numpy()\n",
    "                os_n = output_station_noDenorm.detach().cpu().numpy() if configs.use_norm == 'lt' else None\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(i_n[0, :, -1], label='Original')\n",
    "                plt.plot(t_n[0, :, -1], label='Trend', color='black', linestyle='--')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(i_w[0, :, -1], label='Normalized')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_input.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='Ground Truth', color='black', linewidth=2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(0, label='Before Norm(Model Output)')\n",
    "                plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3)\n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(o_n[0, :, -1], label='Before Norm(Model Output)')\n",
    "                # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_output.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='True')\n",
    "                plt.plot(o_w[0, :, -1], label='Model Output')\n",
    "                if configs.use_norm == 'lt':\n",
    "                    plt.plot(os_n[0, :, -1], label='Station Output')\n",
    "                plt.legend()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(trend_true[0, :, -1], label='Trend True')\n",
    "                if configs.use_norm == 'lt':\n",
    "                    plt.plot(os_n[0, :, -1], label='Trend Predicted')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(folder_path, 'station_output.pdf'))\n",
    "\n",
    "    # if configs.test_flop:\n",
    "    #     test_params_flop((batch_x.shape[1], batch_x.shape[2]))\n",
    "    #     exit()\n",
    "    preds = np.array(preds, dtype=object)\n",
    "    trues = np.array(trues, dtype=object)\n",
    "    # inputx = np.array(inputx)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    # inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "    # result save\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "    print('mse:{}, mae:{}'.format(mse, mae))\n",
    "    f = open(\"result.txt\", 'a')\n",
    "    f.write(setting + \"  \\n\")\n",
    "    f.write('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 4640\n",
      "val 425\n",
      "test 1182\n",
      "\titers: 100, epoch: 1 | loss: 0.3048674\n",
      "\tspeed: 0.0121s/iter; left time: 25.2205s\n",
      "Epoch: 1 cost time: 1.777104377746582\n",
      "Station Epoch: 1, Steps: 145 | Train Loss: 0.2863332 Vali Loss: 0.2366721 Test Loss: 0.1857928\n",
      "Validation loss decreased (inf --> 0.236672).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 2 | loss: 0.2407931\n",
      "\tspeed: 0.0216s/iter; left time: 41.6699s\n",
      "Epoch: 2 cost time: 1.8142235279083252\n",
      "Station Epoch: 2, Steps: 145 | Train Loss: 0.2358221 Vali Loss: 0.2094506 Test Loss: 0.1679239\n",
      "Validation loss decreased (0.236672 --> 0.209451).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2146927\n",
      "\tspeed: 0.0216s/iter; left time: 38.6036s\n",
      "Epoch: 3 cost time: 1.818938970565796\n",
      "Station Epoch: 3, Steps: 145 | Train Loss: 0.2285734 Vali Loss: 0.2081208 Test Loss: 0.1676325\n",
      "Validation loss decreased (0.209451 --> 0.208121).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2468125\n",
      "\tspeed: 0.0214s/iter; left time: 35.0596s\n",
      "Epoch: 4 cost time: 1.7812659740447998\n",
      "Station Epoch: 4, Steps: 145 | Train Loss: 0.2263431 Vali Loss: 0.2041990 Test Loss: 0.1648106\n",
      "Validation loss decreased (0.208121 --> 0.204199).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 5 | loss: 0.2381136\n",
      "\tspeed: 0.0214s/iter; left time: 32.0532s\n",
      "Epoch: 5 cost time: 1.8106625080108643\n",
      "Station Epoch: 5, Steps: 145 | Train Loss: 0.2255552 Vali Loss: 0.2044298 Test Loss: 0.1664520\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 0.3837230\n",
      "\tspeed: 0.0230s/iter; left time: 31.0536s\n",
      "Epoch: 6 cost time: 2.030200719833374\n",
      "Backbone Epoch: 1, Steps: 145 | Train Loss: 0.4527694 Vali Loss: 0.4074454 Test Loss: 0.3303321\n",
      "Validation loss decreased (inf --> 0.407445).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 7 | loss: 0.4307297\n",
      "\tspeed: 0.0240s/iter; left time: 28.9641s\n",
      "Epoch: 7 cost time: 2.083829402923584\n",
      "Backbone Epoch: 2, Steps: 145 | Train Loss: 0.4523244 Vali Loss: 0.4085413 Test Loss: 0.3301160\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.4414259\n",
      "\tspeed: 0.0243s/iter; left time: 25.8078s\n",
      "Epoch: 8 cost time: 2.086484909057617\n",
      "Backbone Epoch: 3, Steps: 145 | Train Loss: 0.4398558 Vali Loss: 0.4295105 Test Loss: 0.3392735\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3946447\n",
      "\tspeed: 0.0252s/iter; left time: 23.1161s\n",
      "Epoch: 9 cost time: 2.2395834922790527\n",
      "Backbone Epoch: 4, Steps: 145 | Train Loss: 0.4208894 Vali Loss: 0.4489514 Test Loss: 0.3438134\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "Average time per pretrain epoch: 2.13908s\n",
      "Average time per joint train epoch: 2.42132s\n",
      "test 1182\n",
      "mse:0.330366849899292, mae:0.41658660769462585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.33036685, 0.4165866)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def make_setting_name(configs):\n",
    "    setting = '{}_{}_pl{}_un{}_dc{}_lr({})_splr({})_sjlr({})_tff{}_dset({})_tepoch{}_tnorm{}_snorm{}'.format(\n",
    "        configs.data_path[:-4],\n",
    "        configs.model_name,\n",
    "        configs.pred_len,\n",
    "        configs.use_norm,\n",
    "        configs.decomp_type,\n",
    "        configs.learning_rate,\n",
    "        configs.station_pre_lr,\n",
    "        configs.station_joint_lr,\n",
    "        configs.t_ff,\n",
    "        configs.alpha if configs.decomp_type == 'ema' else configs.kernel_size,\n",
    "        configs.twice_epoch,\n",
    "        configs.t_norm,\n",
    "        configs.s_norm,\n",
    "    )\n",
    "    return setting\n",
    "\n",
    "setting = make_setting_name(configs)\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ETTh1_iTransformer_pl96_unlt_dcenvelope_lr(0.0001)_splr(1e-05)_sjlr(0.0001)_tff64_dset(25)_tepoch3_tnormTrue_snormFalse'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.3804953098297119, mae:0.4029838442802429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3804953, 0.40298384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 가능한 총 파라미터 수 계산\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in norm.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"모델의 총 학습 가능 파라미터 수: {trainable_params:,} 개\")\n",
    "\n",
    "# 파라미터 저장을 위한 메모리 크기 계산 (기본 float32 기준)\n",
    "# 1 float32 = 4 bytes\n",
    "param_size_bytes = trainable_params * 4\n",
    "param_size_mb = param_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"모델 파라미터의 예상 메모리 크기: {param_size_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7201\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.7581769\n",
      "\tspeed: 0.0053s/iter; left time: 17.3979s\n",
      "\titers: 200, epoch: 1 | loss: 0.9436355\n",
      "\tspeed: 0.0053s/iter; left time: 16.8870s\n",
      "Epoch: 1 cost time: 1.1876552104949951\n",
      "Station Epoch: 1, Steps: 225 | Train Loss: 0.8619922 Vali Loss: 0.7346204 Test Loss: 0.3525207\n",
      "Validation loss decreased (inf --> 0.734620).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 2 | loss: 0.6560441\n",
      "\tspeed: 0.0111s/iter; left time: 33.9457s\n",
      "\titers: 200, epoch: 2 | loss: 0.6961247\n",
      "\tspeed: 0.0054s/iter; left time: 15.8585s\n",
      "Epoch: 2 cost time: 1.2024035453796387\n",
      "Station Epoch: 2, Steps: 225 | Train Loss: 0.8526279 Vali Loss: 0.7229651 Test Loss: 0.3511476\n",
      "Validation loss decreased (0.734620 --> 0.722965).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "\titers: 100, epoch: 3 | loss: 0.7075656\n",
      "\tspeed: 0.0114s/iter; left time: 32.1812s\n",
      "\titers: 200, epoch: 3 | loss: 0.8973257\n",
      "\tspeed: 0.0052s/iter; left time: 14.1843s\n",
      "Epoch: 3 cost time: 1.1764435768127441\n",
      "Station Epoch: 3, Steps: 225 | Train Loss: 0.8455808 Vali Loss: 0.7188644 Test Loss: 0.3504516\n",
      "Validation loss decreased (0.722965 --> 0.718864).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "\titers: 100, epoch: 4 | loss: 0.6242340\n",
      "\tspeed: 0.0109s/iter; left time: 28.4380s\n",
      "\titers: 200, epoch: 4 | loss: 1.3776972\n",
      "\tspeed: 0.0053s/iter; left time: 13.3632s\n",
      "Epoch: 4 cost time: 1.1639304161071777\n",
      "Station Epoch: 4, Steps: 225 | Train Loss: 0.8414394 Vali Loss: 0.7157307 Test Loss: 0.3500996\n",
      "Validation loss decreased (0.718864 --> 0.715731).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "\titers: 100, epoch: 5 | loss: 0.7854842\n",
      "\tspeed: 0.0116s/iter; left time: 27.4570s\n",
      "\titers: 200, epoch: 5 | loss: 1.1309911\n",
      "\tspeed: 0.0054s/iter; left time: 12.3637s\n",
      "Epoch: 5 cost time: 1.2131388187408447\n",
      "Station Epoch: 5, Steps: 225 | Train Loss: 0.8400683 Vali Loss: 0.7150485 Test Loss: 0.3499247\n",
      "Validation loss decreased (0.715731 --> 0.715048).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 1.0100662\n",
      "\tspeed: 0.0240s/iter; left time: 51.6257s\n",
      "\titers: 200, epoch: 6 | loss: 0.6283067\n",
      "\tspeed: 0.0176s/iter; left time: 36.1417s\n",
      "Epoch: 6 cost time: 4.017696380615234\n",
      "Backbone Epoch: 1, Steps: 225 | Train Loss: 0.9767400 Vali Loss: 0.7395227 Test Loss: 0.4435011\n",
      "Validation loss decreased (inf --> 0.739523).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 7 | loss: 1.0917602\n",
      "\tspeed: 0.0322s/iter; left time: 62.0980s\n",
      "\titers: 200, epoch: 7 | loss: 0.6679821\n",
      "\tspeed: 0.0180s/iter; left time: 32.8762s\n",
      "Epoch: 7 cost time: 4.068219184875488\n",
      "Backbone Epoch: 2, Steps: 225 | Train Loss: 0.9089204 Vali Loss: 0.7322854 Test Loss: 0.4288523\n",
      "Validation loss decreased (0.739523 --> 0.732285).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.7607693\n",
      "\tspeed: 0.0330s/iter; left time: 56.2099s\n",
      "\titers: 200, epoch: 8 | loss: 1.1101314\n",
      "\tspeed: 0.0187s/iter; left time: 29.9483s\n",
      "Epoch: 8 cost time: 4.216555118560791\n",
      "Backbone Epoch: 3, Steps: 225 | Train Loss: 0.8916649 Vali Loss: 0.7187335 Test Loss: 0.3999175\n",
      "Validation loss decreased (0.732285 --> 0.718734).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 9 | loss: 1.0895000\n",
      "\tspeed: 0.0313s/iter; left time: 46.2238s\n",
      "\titers: 200, epoch: 9 | loss: 0.6937146\n",
      "\tspeed: 0.0177s/iter; left time: 24.2971s\n",
      "Epoch: 9 cost time: 3.9259064197540283\n",
      "Backbone Epoch: 4, Steps: 225 | Train Loss: 0.8624049 Vali Loss: 0.7083221 Test Loss: 0.3810974\n",
      "Validation loss decreased (0.718734 --> 0.708322).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.8454124\n",
      "\tspeed: 0.0321s/iter; left time: 40.2056s\n",
      "\titers: 200, epoch: 10 | loss: 0.8007569\n",
      "\tspeed: 0.0175s/iter; left time: 20.1722s\n",
      "Epoch: 10 cost time: 4.020641326904297\n",
      "Backbone Epoch: 5, Steps: 225 | Train Loss: 0.8317907 Vali Loss: 0.7071365 Test Loss: 0.3729743\n",
      "Validation loss decreased (0.708322 --> 0.707137).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 11 | loss: 1.2636704\n",
      "\tspeed: 0.0307s/iter; left time: 31.5242s\n",
      "\titers: 200, epoch: 11 | loss: 0.5371707\n",
      "\tspeed: 0.0172s/iter; left time: 15.8832s\n",
      "Epoch: 11 cost time: 3.847550392150879\n",
      "Backbone Epoch: 6, Steps: 225 | Train Loss: 0.8103570 Vali Loss: 0.7118289 Test Loss: 0.3720203\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.8765784\n",
      "\tspeed: 0.0315s/iter; left time: 25.2184s\n",
      "\titers: 200, epoch: 12 | loss: 0.5144888\n",
      "\tspeed: 0.0172s/iter; left time: 12.0404s\n",
      "Epoch: 12 cost time: 3.8610472679138184\n",
      "Backbone Epoch: 7, Steps: 225 | Train Loss: 0.7994788 Vali Loss: 0.7165598 Test Loss: 0.3724926\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.8843291\n",
      "\tspeed: 0.0307s/iter; left time: 17.6630s\n",
      "\titers: 200, epoch: 13 | loss: 0.7674705\n",
      "\tspeed: 0.0171s/iter; left time: 8.1437s\n",
      "Epoch: 13 cost time: 3.8360366821289062\n",
      "Backbone Epoch: 8, Steps: 225 | Train Loss: 0.7953129 Vali Loss: 0.7174545 Test Loss: 0.3723657\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "Average time per pretrain epoch: 1.66244s\n",
      "Average time per joint train epoch: 4.94965s\n",
      "test 2161\n",
      "mse:0.3713555634021759, mae:0.43030044436454773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37135556, 0.43030044)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.root_path='datasets/ETT-small'\n",
    "configs.data='ETTh2'\n",
    "configs.data_path='ETTh2.csv'\n",
    "\n",
    "configs.model_name=\"iTransformer\"\n",
    "if configs.model_name==\"iTransformer\":\n",
    "    configs.d_model=128\n",
    "    configs.d_ff=128\n",
    "    configs.factor=1\n",
    "elif configs.model_name in [\"Autoformer\", \"FEDformer\"]:\n",
    "    configs.d_model=512\n",
    "    configs.d_ff=2048\n",
    "    configs.factor=3\n",
    "    \n",
    "configs.learning_rate=0.0001\n",
    "configs.seq_len=720\n",
    "configs.label_len=168\n",
    "configs.pred_len=720\n",
    "configs.enc_in=7\n",
    "configs.dec_in=7\n",
    "configs.c_out=7\n",
    "\n",
    "configs.use_norm='lt'\n",
    "configs.station_pre_lr=0.00001\n",
    "configs.station_joint_lr=0.00001\n",
    "configs.t_ff=64\n",
    "configs.alpha=0.2\n",
    "configs.twice_epoch=3\n",
    "configs.use_mlp=True\n",
    "configs.t_norm=True\n",
    "configs.s_norm=False\n",
    "configs.affine=True\n",
    "\n",
    "decomp = ema_decomp(configs.alpha)\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_pre_lr)\n",
    "\n",
    "setting = 'test'\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2161\n",
      "mse:0.5507868528366089, mae:0.5359645485877991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.55078685, 0.53596455)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 결과 포맷팅\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('result.csv')\n",
    "result_list = []\n",
    "for i in range(len(df)):\n",
    "    settings = df.Setting[i].split('_')\n",
    "    dataset = settings[1]\n",
    "    model_name = settings[4]\n",
    "    seq_len = settings[7][2:]\n",
    "    pred_len = settings[9][2:]\n",
    "    trend_norm = settings[-4][2:]\n",
    "    station_pre_lr = settings[-3][3:]\n",
    "    use_mlp = settings[-2][2:]\n",
    "    mse = df.MSE[i]\n",
    "    mae = df.MAE[i]\n",
    "    \n",
    "    result_list.append([dataset, model_name, seq_len, pred_len, trend_norm, station_pre_lr, use_mlp, mse, mae])\n",
    "df_result = pd.DataFrame(result_list, columns=['Dataset', 'Model', 'Seq_Len', 'Pred_Len', 'Trend_Norm', 'Station_LR', 'Use_MLP', 'MSE', 'MAE'])\n",
    "df_result.to_csv('summary_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7585\n"
     ]
    }
   ],
   "source": [
    "data_set, data_loader = data_provider(configs, flag='train')\n",
    "\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = next(iter(data_loader))\n",
    "\n",
    "decomp = envelope_decomp(25).to(device)\n",
    "seasonal_x, trend_x = decomp(batch_y)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(batch_y[0, :, -1].cpu().numpy(), label='Original Series')\n",
    "plt.plot(trend_x[0, :, -1].cpu().numpy(), label='Extracted Trend', linestyle='--')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(seasonal_x[0, :, -1].cpu().numpy(), label='Extracted Seasonal')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.savefig('decomposition_example.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
