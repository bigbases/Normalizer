{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from utils.metrics import metric\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from models import DLinear, iTransformer, Autoformer, FEDformer, Linear\n",
    "from normalizers import SAN, DDN, RevIN, LightTrend, NoNorm\n",
    "from layers.decomposition import series_decomp, ema_decomp, envelope_decomp\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    task_name = \"long_term_forecast\",\n",
    "    model_name = \"iTransformer\", \n",
    "    \n",
    "    # Task\n",
    "    seq_len=720,\n",
    "    label_len=168,\n",
    "    pred_len=336,\n",
    "    enc_in=321,\n",
    "    dec_in=321,\n",
    "    c_out=321,\n",
    "    features='M',\n",
    "    freq='h',\n",
    "    \n",
    "    # Data\n",
    "    root_path='datasets',\n",
    "    data='custom',\n",
    "    data_path='electricity.csv',\n",
    "    target='OT',\n",
    "    batch_size=16,\n",
    "    \n",
    "    # Basic Model Settings\n",
    "    checkpoints='./manual_exp/checkpoints/',\n",
    "    dropout=0.1,\n",
    "    embed='timeF',\n",
    "    output_attention=False,\n",
    "    activation='gelu',\n",
    "    moving_avg=25,\n",
    "    num_kernels=6,\n",
    "    individual=False,\n",
    "    learning_rate=0.0005,\n",
    "    num_workers=16,\n",
    "    n_heads=8,\n",
    "    patience=3,\n",
    "    train_epochs=10,\n",
    "    use_amp=False,\n",
    "    lradj='type1',\n",
    "    embed_type=0,\n",
    "    e_layers=3,\n",
    "    factor=3,\n",
    "    # d_model=512,\n",
    "    # d_ff=2048,\n",
    "    \n",
    "    # iTransformer\n",
    "    # d_model=128,\n",
    "    # d_ff=128,\n",
    "    d_model=512,\n",
    "    d_ff=512,\n",
    "    class_strategy='projection',\n",
    "    \n",
    "    # Autoformer & FEDformer\n",
    "    # factor=3,\n",
    "    # e_layers=2,\n",
    "    d_layers=1,\n",
    "    \n",
    "    # Normalizer\n",
    "    use_norm='lt',\n",
    "    decomp_type='sma',\n",
    "    norm_variant='original',\n",
    "    station_type='adaptive',\n",
    "    \n",
    "    kernel_len=25,\n",
    "    station_joint_lr=0.0005,\n",
    "    station_pre_lr=0.0001,\n",
    "    t_ff=128,\n",
    "    alpha=0.04,\n",
    "    twice_epoch=3,\n",
    "    use_mlp=True,\n",
    "    t_norm=True,\n",
    "    s_norm=False,\n",
    "    affine=True,\n",
    "    \n",
    "    period_len=24,\n",
    "    pre_epoch=5,\n",
    "    j=1,\n",
    "    learnable=False,\n",
    "    wavelet='coif3',\n",
    "    dr=0.05,\n",
    "    hkernel_len=5,\n",
    "    pd_ff=128,\n",
    "    pd_model=128,\n",
    "    pe_layers=0,\n",
    "    reinit_mean_pretrain=False,\n",
    "    reinit_std_pretrain=False,\n",
    ")\n",
    "model_dict={\n",
    "    'DLinear': DLinear,\n",
    "    'iTransformer': iTransformer,\n",
    "    'Autoformer': Autoformer,\n",
    "    'FEDformer': FEDformer,\n",
    "    'Linear': Linear,\n",
    "}\n",
    "linear_models = ['DLinear', 'Linear']\n",
    "norm_dict = {\n",
    "    'none': NoNorm,\n",
    "    'revin': RevIN,\n",
    "    'san': SAN,\n",
    "    'ddn': DDN,\n",
    "    'lt': LightTrend,\n",
    "}\n",
    "\n",
    "if configs.decomp_type == 'sma':\n",
    "    decomp = series_decomp(configs.kernel_size)\n",
    "elif configs.decomp_type == 'ema':\n",
    "    decomp = ema_decomp(configs.alpha)\n",
    "elif configs.decomp_type == 'envelope':\n",
    "    decomp = envelope_decomp(configs.kernel_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "norm_criterion = nn.MSELoss()\n",
    "# norm_criterion = nn.L1Loss()\n",
    "\n",
    "# SAN\n",
    "def san_loss(y, statistics_pred):\n",
    "    bs, len, dim = y.shape\n",
    "    y = y.reshape(bs, -1, configs.period_len, dim)\n",
    "    mean = torch.mean(y, dim=2)\n",
    "    std = torch.std(y, dim=2)\n",
    "    station_ture = torch.cat([mean, std], dim=-1)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# DDN\n",
    "def ddn_loss(y, statistics_pred):\n",
    "    _, (mean, std) = norm.norm(y.transpose(-1, -2), False)\n",
    "    station_ture = torch.cat([mean, std], dim=1).transpose(-1, -2)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# TREAD\n",
    "def tread_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred[-1]\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "# LightTrend\n",
    "def lt_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = norm_criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "station_loss_dict = {\n",
    "    'none': None,\n",
    "    'revin': None,\n",
    "    'san': san_loss,\n",
    "    'ddn': ddn_loss,\n",
    "    'san_exp': san_loss,\n",
    "    'ddn_exp': ddn_loss,\n",
    "    'tp': tread_loss,\n",
    "    'tp2': tread_loss,\n",
    "    'lt': lt_loss,\n",
    "}\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "\n",
    "# [pre train, pre epoch, joint train, join epoch]\n",
    "station_setting_dict = {\n",
    "    'none': [0, 0, 0, 0],\n",
    "    'revin': [0, 0, 0, 0],\n",
    "    'san': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp2': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    # 'lt': [1, configs.pre_epoch, 0, 0],\n",
    "    'lt': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'san_exp': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn_exp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "}\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_pre_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion, epoch):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "\n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float()\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "            total_loss.append(loss.cpu().item())\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    norm.train()\n",
    "    return total_loss\n",
    "\n",
    "def _get_data(flag):\n",
    "    data_set, data_loader = data_provider(configs, flag)\n",
    "    return data_set, data_loader\n",
    "\n",
    "def train(setting):\n",
    "    train_data, train_loader = _get_data(flag='train')\n",
    "    vali_data, vali_loader = _get_data(flag='val')\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    path = os.path.join(configs.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    path_station = './station/' + '{}_s{}_p{}'.format(configs.use_norm, configs.data,\n",
    "                                                        configs.seq_len, configs.pred_len)\n",
    "    if not os.path.exists(path_station):\n",
    "        os.makedirs(path_station)\n",
    "\n",
    "    time_now = time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "    early_stopping_station_model = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "\n",
    "    if configs.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    ### NEW ###\n",
    "    # 재초기화를 위한 헬퍼 함수 정의\n",
    "    def reinit_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters() # PyTorch 기본값으로 가중치 초기화\n",
    "    ### END NEW ###\n",
    "\n",
    "    time_per_epoch = []\n",
    "    for epoch in range(configs.train_epochs + station_setting[1]):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        epoch_start = time()\n",
    "        # Load best station model after pretraining\n",
    "        if station_setting[0] > 0 and epoch == station_setting[1]:\n",
    "            best_model_path = path_station + '/' + 'checkpoint.pth'\n",
    "            norm.load_state_dict(torch.load(best_model_path))\n",
    "            print('loading pretrained adaptive station model')\n",
    "            \n",
    "            ### NEW ###\n",
    "            # --- Motivating Experiment 3.2: 학습 분리 효과 검증 ---\n",
    "            # 'DDN + w/o trend pretrain' 실험\n",
    "            if getattr(configs, 'reinit_mean_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing MEAN predictor weights !!!\")\n",
    "                norm.mlp.m_project.apply(reinit_weights)\n",
    "                norm.mlp.mean_proj.apply(reinit_weights)\n",
    "                norm.mlp.m_concat.apply(reinit_weights)\n",
    "                norm.mlp.mean_ffn.apply(reinit_weights)\n",
    "                norm.mlp.mean_pred.apply(reinit_weights)\n",
    "                print(\"!!! Mean predictor re-initialization complete. !!!\")\n",
    "\n",
    "            # 'DDN + w/o seasonal pretrain' (std) 실험\n",
    "            if getattr(configs, 'reinit_std_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing STD predictor weights !!!\")\n",
    "                norm.mlp.s_project.apply(reinit_weights)\n",
    "                norm.mlp.std_proj.apply(reinit_weights)\n",
    "                norm.mlp.s_concat.apply(reinit_weights)\n",
    "                norm.mlp.std_ffn.apply(reinit_weights)\n",
    "                norm.mlp.std_pred.apply(reinit_weights)\n",
    "                print(\"!!! Std predictor re-initialization complete. !!!\")\n",
    "            \n",
    "            # 'DDN + w/o pretrain' 실험은 station_setting[1] = 0 으로 설정하고\n",
    "            # 이 if 블록에 들어오지 않게 함으로써 자연스럽게 구현됩니다.\n",
    "            ### END NEW ###\n",
    "        \n",
    "        # Add station parameters to model optim after pretraining and delay epochs for joint training\n",
    "        if station_setting[2] > 0 and station_setting[3] == epoch - station_setting[1] + 1:\n",
    "            # lr = model_optim.param_groups[0]['lr']\n",
    "            lr = configs.station_joint_lr\n",
    "            # lr = norm_optim.param_groups[0]['lr']\n",
    "            model_optim.add_param_group({'params': norm.parameters(), 'lr': lr})\n",
    "        \n",
    "        model.train()\n",
    "        norm.train()\n",
    "        epoch_time = time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            \n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                        f_dim = -1 if configs.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                    f_dim = -1 if configs.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                    if configs.features == 'MS':\n",
    "                        statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                        \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time() - time_now) / iter_count\n",
    "                left_time = speed * (\n",
    "                        (configs.train_epochs + station_setting[1] - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time()\n",
    "            if configs.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                # two-stage training schema\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    norm_optim.step()\n",
    "                else:\n",
    "                    model_optim.step()\n",
    "                model_optim.zero_grad()\n",
    "                norm_optim.zero_grad()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion, epoch)\n",
    "        test_loss = vali(test_data, test_loader, criterion, epoch)\n",
    "\n",
    "        if epoch + 1 <= station_setting[1]:\n",
    "            print(\n",
    "                \"Station Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping_station_model(vali_loss, norm, path_station)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1, configs, configs.station_pre_lr)\n",
    "        else:\n",
    "            print(\n",
    "                \"Backbone Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1 - station_setting[1], train_steps, train_loss, vali_loss, test_loss))\n",
    "            # if: joint training, else: only model training\n",
    "            if station_setting[2] > 0 and station_setting[3] <= epoch - station_setting[1]:\n",
    "                early_stopping(vali_loss, model, path, norm, path_station)\n",
    "            else:\n",
    "                early_stopping(vali_loss, model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(model_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.learning_rate)\n",
    "            # adjust_learning_rate(norm_optim, epoch + 1 - station_setting[1], configs,\n",
    "            #                         configs.station_pre_lr)\n",
    "        time_per_epoch.append(time() - epoch_start)\n",
    "    print(\"Average time per pretrain epoch: {:.5f}s\".format(np.mean(time_per_epoch[:5])))\n",
    "    print(\"Average time per joint train epoch: {:.5f}s\".format(np.mean(time_per_epoch[5:])))\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if station_setting[2] > 0:\n",
    "        norm.load_state_dict(torch.load(path_station + '/' + 'checkpoint.pth'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(setting, test=0):\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    if test:\n",
    "        print('loading model')\n",
    "        model.load_state_dict(torch.load(os.path.join('./manual_exp/checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    inputx = []\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            input_x = batch_x\n",
    "\n",
    "            # normalize\n",
    "            input_noNorm = batch_x.clone()\n",
    "            _, trend_noNorm = decomp(input_noNorm)\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            input_withNorm = batch_x\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder x\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "            dec_label = batch_x[:, -configs.label_len:, :]\n",
    "            dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "            # encoder - decoder\n",
    "            if configs.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            else:\n",
    "                if configs.model_name in linear_models:\n",
    "                    outputs = model(batch_x)\n",
    "                else:\n",
    "                    if configs.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "            f_dim = -1 if configs.features == 'MS' else 0\n",
    "            outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "            if configs.features == 'MS':\n",
    "                statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                \n",
    "            # de-normalize\n",
    "            output_noDenorm = outputs.clone()\n",
    "            outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "            output_withDenorm = outputs\n",
    "            \n",
    "            batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "            _, trend_true = decomp(batch_y)\n",
    "            trend_true = trend_true.detach().cpu().numpy()\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "            pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "            true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "            \n",
    "            # LightTrend가 예측한 추세 시각화\n",
    "            output_station_noDenorm = statistics_pred.clone() if configs.use_norm == 'lt' else None\n",
    "\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "            inputx.append(batch_x.detach().cpu().numpy())\n",
    "            if i % 20 == 0:\n",
    "                x = input_x.detach().cpu().numpy()\n",
    "                gt = np.concatenate((x[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                pd = np.concatenate((x[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "            \n",
    "            # visualize input and output with/without normalization/denormalization\n",
    "            if i == 0:\n",
    "                i_n = input_noNorm.detach().cpu().numpy()\n",
    "                t_n = trend_noNorm.detach().cpu().numpy()\n",
    "                i_w = input_withNorm.detach().cpu().numpy()\n",
    "                o_n = output_noDenorm.detach().cpu().numpy()\n",
    "                o_w = output_withDenorm.detach().cpu().numpy()\n",
    "                os_n = output_station_noDenorm.detach().cpu().numpy() if configs.use_norm == 'lt' else None\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(i_n[0, :, -1], label='Original')\n",
    "                plt.plot(t_n[0, :, -1], label='Trend', color='black', linestyle='--')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(i_w[0, :, -1], label='Normalized')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_input.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='Ground Truth', color='black', linewidth=2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(0, label='Before Norm(Model Output)')\n",
    "                plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3)\n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(o_n[0, :, -1], label='Before Norm(Model Output)')\n",
    "                # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_output.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='True')\n",
    "                plt.plot(o_w[0, :, -1], label='Model Output')\n",
    "                if configs.use_norm == 'lt':\n",
    "                    plt.plot(os_n[0, :, -1], label='Station Output')\n",
    "                plt.legend()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(trend_true[0, :, -1], label='Trend True')\n",
    "                if configs.use_norm == 'lt':\n",
    "                    plt.plot(os_n[0, :, -1], label='Trend Predicted')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(folder_path, 'station_output.pdf'))\n",
    "\n",
    "    # if configs.test_flop:\n",
    "    #     test_params_flop((batch_x.shape[1], batch_x.shape[2]))\n",
    "    #     exit()\n",
    "    preds = np.array(preds, dtype=object)\n",
    "    trues = np.array(trues, dtype=object)\n",
    "    # inputx = np.array(inputx)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    # inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "    # result save\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "    print('mse:{}, mae:{}'.format(mse, mae))\n",
    "    f = open(\"result.txt\", 'a')\n",
    "    f.write(setting + \"  \\n\")\n",
    "    f.write('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 17357\n",
      "val 2297\n",
      "test 4925\n",
      "\titers: 100, epoch: 1 | loss: 0.1119439\n",
      "\tspeed: 0.0163s/iter; left time: 264.1622s\n",
      "\titers: 200, epoch: 1 | loss: 0.1241128\n",
      "\tspeed: 0.0160s/iter; left time: 257.7640s\n",
      "\titers: 300, epoch: 1 | loss: 0.1084009\n",
      "\tspeed: 0.0161s/iter; left time: 257.0612s\n",
      "\titers: 400, epoch: 1 | loss: 0.0966522\n",
      "\tspeed: 0.0156s/iter; left time: 247.6198s\n",
      "\titers: 500, epoch: 1 | loss: 0.1105144\n",
      "\tspeed: 0.0162s/iter; left time: 255.6397s\n",
      "\titers: 600, epoch: 1 | loss: 0.1109987\n",
      "\tspeed: 0.0162s/iter; left time: 253.2387s\n",
      "\titers: 700, epoch: 1 | loss: 0.1068911\n",
      "\tspeed: 0.0165s/iter; left time: 257.1145s\n",
      "\titers: 800, epoch: 1 | loss: 0.0880325\n",
      "\tspeed: 0.0157s/iter; left time: 242.1471s\n",
      "\titers: 900, epoch: 1 | loss: 0.0765597\n",
      "\tspeed: 0.0162s/iter; left time: 249.4547s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1182397\n",
      "\tspeed: 0.0158s/iter; left time: 241.7820s\n",
      "Epoch: 1 cost time: 17.4562828540802\n",
      "Station Epoch: 1, Steps: 1084 | Train Loss: 0.1036003 Vali Loss: 0.0660159 Test Loss: 0.0906525\n",
      "Validation loss decreased (inf --> 0.066016).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1055561\n",
      "\tspeed: 0.0848s/iter; left time: 1277.9692s\n",
      "\titers: 200, epoch: 2 | loss: 0.0703518\n",
      "\tspeed: 0.0159s/iter; left time: 238.8662s\n",
      "\titers: 300, epoch: 2 | loss: 0.1102557\n",
      "\tspeed: 0.0158s/iter; left time: 235.4468s\n",
      "\titers: 400, epoch: 2 | loss: 0.0718076\n",
      "\tspeed: 0.0157s/iter; left time: 231.7928s\n",
      "\titers: 500, epoch: 2 | loss: 0.0680549\n",
      "\tspeed: 0.0164s/iter; left time: 240.5069s\n",
      "\titers: 600, epoch: 2 | loss: 0.0896660\n",
      "\tspeed: 0.0159s/iter; left time: 231.6024s\n",
      "\titers: 700, epoch: 2 | loss: 0.0974014\n",
      "\tspeed: 0.0164s/iter; left time: 237.3985s\n",
      "\titers: 800, epoch: 2 | loss: 0.0908292\n",
      "\tspeed: 0.0176s/iter; left time: 253.5498s\n",
      "\titers: 900, epoch: 2 | loss: 0.0838059\n",
      "\tspeed: 0.0165s/iter; left time: 235.3476s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1071028\n",
      "\tspeed: 0.0158s/iter; left time: 223.9094s\n",
      "Epoch: 2 cost time: 17.6031813621521\n",
      "Station Epoch: 2, Steps: 1084 | Train Loss: 0.0902620 Vali Loss: 0.0634606 Test Loss: 0.0874853\n",
      "Validation loss decreased (0.066016 --> 0.063461).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0841561\n",
      "\tspeed: 0.0838s/iter; left time: 1172.8872s\n",
      "\titers: 200, epoch: 3 | loss: 0.0729625\n",
      "\tspeed: 0.0159s/iter; left time: 220.4307s\n",
      "\titers: 300, epoch: 3 | loss: 0.0838957\n",
      "\tspeed: 0.0158s/iter; left time: 218.4233s\n",
      "\titers: 400, epoch: 3 | loss: 0.0966823\n",
      "\tspeed: 0.0158s/iter; left time: 216.1568s\n",
      "\titers: 500, epoch: 3 | loss: 0.0842583\n",
      "\tspeed: 0.0162s/iter; left time: 220.3542s\n",
      "\titers: 600, epoch: 3 | loss: 0.0916464\n",
      "\tspeed: 0.0165s/iter; left time: 222.7020s\n",
      "\titers: 700, epoch: 3 | loss: 0.1226981\n",
      "\tspeed: 0.0169s/iter; left time: 226.5111s\n",
      "\titers: 800, epoch: 3 | loss: 0.0926475\n",
      "\tspeed: 0.0171s/iter; left time: 227.3856s\n",
      "\titers: 900, epoch: 3 | loss: 0.0664054\n",
      "\tspeed: 0.0165s/iter; left time: 218.2891s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0633232\n",
      "\tspeed: 0.0162s/iter; left time: 211.8793s\n",
      "Epoch: 3 cost time: 17.59274125099182\n",
      "Station Epoch: 3, Steps: 1084 | Train Loss: 0.0882585 Vali Loss: 0.0630113 Test Loss: 0.0869618\n",
      "Validation loss decreased (0.063461 --> 0.063011).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0736612\n",
      "\tspeed: 0.0845s/iter; left time: 1090.9376s\n",
      "\titers: 200, epoch: 4 | loss: 0.0785396\n",
      "\tspeed: 0.0159s/iter; left time: 204.1583s\n",
      "\titers: 300, epoch: 4 | loss: 0.0982728\n",
      "\tspeed: 0.0157s/iter; left time: 199.9714s\n",
      "\titers: 400, epoch: 4 | loss: 0.0901378\n",
      "\tspeed: 0.0162s/iter; left time: 204.1301s\n",
      "\titers: 500, epoch: 4 | loss: 0.0750549\n",
      "\tspeed: 0.0161s/iter; left time: 201.8090s\n",
      "\titers: 600, epoch: 4 | loss: 0.0916808\n",
      "\tspeed: 0.0162s/iter; left time: 201.2393s\n",
      "\titers: 700, epoch: 4 | loss: 0.0853798\n",
      "\tspeed: 0.0166s/iter; left time: 204.6053s\n",
      "\titers: 800, epoch: 4 | loss: 0.0720994\n",
      "\tspeed: 0.0163s/iter; left time: 198.9676s\n",
      "\titers: 900, epoch: 4 | loss: 0.0831141\n",
      "\tspeed: 0.0164s/iter; left time: 198.5354s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1418744\n",
      "\tspeed: 0.0159s/iter; left time: 190.5993s\n",
      "Epoch: 4 cost time: 17.466402769088745\n",
      "Station Epoch: 4, Steps: 1084 | Train Loss: 0.0876935 Vali Loss: 0.0627662 Test Loss: 0.0867283\n",
      "Validation loss decreased (0.063011 --> 0.062766).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1348680\n",
      "\tspeed: 0.0855s/iter; left time: 1011.3332s\n",
      "\titers: 200, epoch: 5 | loss: 0.0972654\n",
      "\tspeed: 0.0160s/iter; left time: 187.7109s\n",
      "\titers: 300, epoch: 5 | loss: 0.0828617\n",
      "\tspeed: 0.0161s/iter; left time: 187.3554s\n",
      "\titers: 400, epoch: 5 | loss: 0.0828374\n",
      "\tspeed: 0.0158s/iter; left time: 182.2249s\n",
      "\titers: 500, epoch: 5 | loss: 0.0876820\n",
      "\tspeed: 0.0157s/iter; left time: 178.9983s\n",
      "\titers: 600, epoch: 5 | loss: 0.0965824\n",
      "\tspeed: 0.0157s/iter; left time: 177.9350s\n",
      "\titers: 700, epoch: 5 | loss: 0.0821791\n",
      "\tspeed: 0.0158s/iter; left time: 176.8411s\n",
      "\titers: 800, epoch: 5 | loss: 0.0787197\n",
      "\tspeed: 0.0158s/iter; left time: 175.3542s\n",
      "\titers: 900, epoch: 5 | loss: 0.0625472\n",
      "\tspeed: 0.0158s/iter; left time: 174.4865s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0703694\n",
      "\tspeed: 0.0158s/iter; left time: 172.6285s\n",
      "Epoch: 5 cost time: 17.28745460510254\n",
      "Station Epoch: 5, Steps: 1084 | Train Loss: 0.0874399 Vali Loss: 0.0626186 Test Loss: 0.0866372\n",
      "Validation loss decreased (0.062766 --> 0.062619).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 0.2033620\n",
      "\tspeed: 0.1188s/iter; left time: 1275.9489s\n",
      "\titers: 200, epoch: 6 | loss: 0.1817765\n",
      "\tspeed: 0.0512s/iter; left time: 544.4288s\n",
      "\titers: 300, epoch: 6 | loss: 0.2207003\n",
      "\tspeed: 0.0512s/iter; left time: 539.7053s\n",
      "\titers: 400, epoch: 6 | loss: 0.1979531\n",
      "\tspeed: 0.0516s/iter; left time: 538.4020s\n",
      "\titers: 500, epoch: 6 | loss: 0.1982259\n",
      "\tspeed: 0.0518s/iter; left time: 535.6525s\n",
      "\titers: 600, epoch: 6 | loss: 0.1846443\n",
      "\tspeed: 0.0517s/iter; left time: 529.4606s\n",
      "\titers: 700, epoch: 6 | loss: 0.2033935\n",
      "\tspeed: 0.0517s/iter; left time: 523.9725s\n",
      "\titers: 800, epoch: 6 | loss: 0.1948021\n",
      "\tspeed: 0.0518s/iter; left time: 520.3175s\n",
      "\titers: 900, epoch: 6 | loss: 0.1616475\n",
      "\tspeed: 0.0517s/iter; left time: 514.0017s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1898227\n",
      "\tspeed: 0.0517s/iter; left time: 509.1993s\n",
      "Epoch: 6 cost time: 55.85769248008728\n",
      "Backbone Epoch: 1, Steps: 1084 | Train Loss: 0.2011598 Vali Loss: 0.1484098 Test Loss: 0.1751066\n",
      "Validation loss decreased (inf --> 0.148410).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 7 | loss: 0.1837132\n",
      "\tspeed: 0.2170s/iter; left time: 2095.7485s\n",
      "\titers: 200, epoch: 7 | loss: 0.1679014\n",
      "\tspeed: 0.0519s/iter; left time: 495.8205s\n",
      "\titers: 300, epoch: 7 | loss: 0.1518662\n",
      "\tspeed: 0.0519s/iter; left time: 490.8579s\n",
      "\titers: 400, epoch: 7 | loss: 0.1783370\n",
      "\tspeed: 0.0518s/iter; left time: 484.5297s\n",
      "\titers: 500, epoch: 7 | loss: 0.1570814\n",
      "\tspeed: 0.0518s/iter; left time: 479.7783s\n",
      "\titers: 600, epoch: 7 | loss: 0.2052530\n",
      "\tspeed: 0.0518s/iter; left time: 474.7203s\n",
      "\titers: 700, epoch: 7 | loss: 0.1572807\n",
      "\tspeed: 0.0519s/iter; left time: 469.9132s\n",
      "\titers: 800, epoch: 7 | loss: 0.1884992\n",
      "\tspeed: 0.0520s/iter; left time: 466.0724s\n",
      "\titers: 900, epoch: 7 | loss: 0.1627386\n",
      "\tspeed: 0.0519s/iter; left time: 459.7630s\n",
      "\titers: 1000, epoch: 7 | loss: 0.1582543\n",
      "\tspeed: 0.0519s/iter; left time: 454.7707s\n",
      "Epoch: 7 cost time: 56.22368240356445\n",
      "Backbone Epoch: 2, Steps: 1084 | Train Loss: 0.1782386 Vali Loss: 0.1456624 Test Loss: 0.1686694\n",
      "Validation loss decreased (0.148410 --> 0.145662).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 8 | loss: 0.1685343\n",
      "\tspeed: 0.2167s/iter; left time: 1858.0004s\n",
      "\titers: 200, epoch: 8 | loss: 0.1853523\n",
      "\tspeed: 0.0522s/iter; left time: 442.2547s\n",
      "\titers: 300, epoch: 8 | loss: 0.1658623\n",
      "\tspeed: 0.0523s/iter; left time: 437.7318s\n",
      "\titers: 400, epoch: 8 | loss: 0.1535486\n",
      "\tspeed: 0.0522s/iter; left time: 431.6776s\n",
      "\titers: 500, epoch: 8 | loss: 0.1742875\n",
      "\tspeed: 0.0520s/iter; left time: 425.4015s\n",
      "\titers: 600, epoch: 8 | loss: 0.1835687\n",
      "\tspeed: 0.0520s/iter; left time: 419.4476s\n",
      "\titers: 700, epoch: 8 | loss: 0.1605002\n",
      "\tspeed: 0.0520s/iter; left time: 414.3019s\n",
      "\titers: 800, epoch: 8 | loss: 0.1715319\n",
      "\tspeed: 0.0520s/iter; left time: 409.5284s\n",
      "\titers: 900, epoch: 8 | loss: 0.1603213\n",
      "\tspeed: 0.0520s/iter; left time: 404.4023s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1613826\n",
      "\tspeed: 0.0521s/iter; left time: 399.8869s\n",
      "Epoch: 8 cost time: 56.43949246406555\n",
      "Backbone Epoch: 3, Steps: 1084 | Train Loss: 0.1657842 Vali Loss: 0.1406680 Test Loss: 0.1638867\n",
      "Validation loss decreased (0.145662 --> 0.140668).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 9 | loss: 0.1953999\n",
      "\tspeed: 0.2167s/iter; left time: 1622.8988s\n",
      "\titers: 200, epoch: 9 | loss: 0.1269704\n",
      "\tspeed: 0.0522s/iter; left time: 386.0481s\n",
      "\titers: 300, epoch: 9 | loss: 0.1488591\n",
      "\tspeed: 0.0521s/iter; left time: 380.0555s\n",
      "\titers: 400, epoch: 9 | loss: 0.1573466\n",
      "\tspeed: 0.0520s/iter; left time: 374.1790s\n",
      "\titers: 500, epoch: 9 | loss: 0.1872292\n",
      "\tspeed: 0.0520s/iter; left time: 368.6582s\n",
      "\titers: 600, epoch: 9 | loss: 0.1525963\n",
      "\tspeed: 0.0521s/iter; left time: 364.1511s\n",
      "\titers: 700, epoch: 9 | loss: 0.1588717\n",
      "\tspeed: 0.0520s/iter; left time: 358.4103s\n",
      "\titers: 800, epoch: 9 | loss: 0.1378667\n",
      "\tspeed: 0.0519s/iter; left time: 352.6378s\n",
      "\titers: 900, epoch: 9 | loss: 0.1631114\n",
      "\tspeed: 0.0519s/iter; left time: 347.4536s\n",
      "\titers: 1000, epoch: 9 | loss: 0.1571239\n",
      "\tspeed: 0.0520s/iter; left time: 342.5633s\n",
      "Epoch: 9 cost time: 56.3948814868927\n",
      "Backbone Epoch: 4, Steps: 1084 | Train Loss: 0.1560575 Vali Loss: 0.1370207 Test Loss: 0.1603779\n",
      "Validation loss decreased (0.140668 --> 0.137021).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.1476799\n",
      "\tspeed: 0.2163s/iter; left time: 1385.4494s\n",
      "\titers: 200, epoch: 10 | loss: 0.1578234\n",
      "\tspeed: 0.0519s/iter; left time: 327.0055s\n",
      "\titers: 300, epoch: 10 | loss: 0.1622723\n",
      "\tspeed: 0.0521s/iter; left time: 323.1842s\n",
      "\titers: 400, epoch: 10 | loss: 0.1487216\n",
      "\tspeed: 0.0523s/iter; left time: 319.1511s\n",
      "\titers: 500, epoch: 10 | loss: 0.1405711\n",
      "\tspeed: 0.0521s/iter; left time: 312.8071s\n",
      "\titers: 600, epoch: 10 | loss: 0.1797344\n",
      "\tspeed: 0.0523s/iter; left time: 308.7439s\n",
      "\titers: 700, epoch: 10 | loss: 0.1564320\n",
      "\tspeed: 0.0521s/iter; left time: 302.3037s\n",
      "\titers: 800, epoch: 10 | loss: 0.1305401\n",
      "\tspeed: 0.0519s/iter; left time: 295.9955s\n",
      "\titers: 900, epoch: 10 | loss: 0.1593553\n",
      "\tspeed: 0.0520s/iter; left time: 291.2990s\n",
      "\titers: 1000, epoch: 10 | loss: 0.1421322\n",
      "\tspeed: 0.0519s/iter; left time: 285.8056s\n",
      "Epoch: 10 cost time: 56.39449095726013\n",
      "Backbone Epoch: 5, Steps: 1084 | Train Loss: 0.1519599 Vali Loss: 0.1374058 Test Loss: 0.1595401\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.1398115\n",
      "\tspeed: 0.2176s/iter; left time: 1157.7664s\n",
      "\titers: 200, epoch: 11 | loss: 0.1612606\n",
      "\tspeed: 0.0519s/iter; left time: 270.7148s\n",
      "\titers: 300, epoch: 11 | loss: 0.1525304\n",
      "\tspeed: 0.0520s/iter; left time: 266.0454s\n",
      "\titers: 400, epoch: 11 | loss: 0.1831151\n",
      "\tspeed: 0.0518s/iter; left time: 260.2764s\n",
      "\titers: 500, epoch: 11 | loss: 0.1352604\n",
      "\tspeed: 0.0520s/iter; left time: 255.7645s\n",
      "\titers: 600, epoch: 11 | loss: 0.1539634\n",
      "\tspeed: 0.0519s/iter; left time: 250.3478s\n",
      "\titers: 700, epoch: 11 | loss: 0.1541132\n",
      "\tspeed: 0.0521s/iter; left time: 245.7505s\n",
      "\titers: 800, epoch: 11 | loss: 0.1446580\n",
      "\tspeed: 0.0519s/iter; left time: 240.0419s\n",
      "\titers: 900, epoch: 11 | loss: 0.1574055\n",
      "\tspeed: 0.0519s/iter; left time: 234.8378s\n",
      "\titers: 1000, epoch: 11 | loss: 0.1359021\n",
      "\tspeed: 0.0520s/iter; left time: 229.9360s\n",
      "Epoch: 11 cost time: 56.29573702812195\n",
      "Backbone Epoch: 6, Steps: 1084 | Train Loss: 0.1498391 Vali Loss: 0.1367788 Test Loss: 0.1601001\n",
      "Validation loss decreased (0.137021 --> 0.136779).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.1406721\n",
      "\tspeed: 0.2170s/iter; left time: 919.4470s\n",
      "\titers: 200, epoch: 12 | loss: 0.1406096\n",
      "\tspeed: 0.0519s/iter; left time: 214.6176s\n",
      "\titers: 300, epoch: 12 | loss: 0.1407397\n",
      "\tspeed: 0.0519s/iter; left time: 209.3562s\n",
      "\titers: 400, epoch: 12 | loss: 0.1386881\n",
      "\tspeed: 0.0519s/iter; left time: 204.2680s\n",
      "\titers: 500, epoch: 12 | loss: 0.1486616\n",
      "\tspeed: 0.0520s/iter; left time: 199.5386s\n",
      "\titers: 600, epoch: 12 | loss: 0.1280597\n",
      "\tspeed: 0.0520s/iter; left time: 194.2044s\n",
      "\titers: 700, epoch: 12 | loss: 0.1616402\n",
      "\tspeed: 0.0520s/iter; left time: 189.0240s\n",
      "\titers: 800, epoch: 12 | loss: 0.1464809\n",
      "\tspeed: 0.0521s/iter; left time: 184.4505s\n",
      "\titers: 900, epoch: 12 | loss: 0.1351332\n",
      "\tspeed: 0.0520s/iter; left time: 178.7483s\n",
      "\titers: 1000, epoch: 12 | loss: 0.1323148\n",
      "\tspeed: 0.0520s/iter; left time: 173.4369s\n",
      "Epoch: 12 cost time: 56.314971923828125\n",
      "Backbone Epoch: 7, Steps: 1084 | Train Loss: 0.1486294 Vali Loss: 0.1370395 Test Loss: 0.1601871\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.1553991\n",
      "\tspeed: 0.2169s/iter; left time: 683.7286s\n",
      "\titers: 200, epoch: 13 | loss: 0.1334243\n",
      "\tspeed: 0.0518s/iter; left time: 158.2560s\n",
      "\titers: 300, epoch: 13 | loss: 0.1611293\n",
      "\tspeed: 0.0519s/iter; left time: 153.3886s\n",
      "\titers: 400, epoch: 13 | loss: 0.1597192\n",
      "\tspeed: 0.0520s/iter; left time: 148.3012s\n",
      "\titers: 500, epoch: 13 | loss: 0.1624376\n",
      "\tspeed: 0.0519s/iter; left time: 142.8407s\n",
      "\titers: 600, epoch: 13 | loss: 0.1416423\n",
      "\tspeed: 0.0518s/iter; left time: 137.4372s\n",
      "\titers: 700, epoch: 13 | loss: 0.1403524\n",
      "\tspeed: 0.0519s/iter; left time: 132.4939s\n",
      "\titers: 800, epoch: 13 | loss: 0.1466664\n",
      "\tspeed: 0.0519s/iter; left time: 127.2171s\n",
      "\titers: 900, epoch: 13 | loss: 0.1488569\n",
      "\tspeed: 0.0521s/iter; left time: 122.5543s\n",
      "\titers: 1000, epoch: 13 | loss: 0.1554209\n",
      "\tspeed: 0.0520s/iter; left time: 117.1519s\n",
      "Epoch: 13 cost time: 56.26193857192993\n",
      "Backbone Epoch: 8, Steps: 1084 | Train Loss: 0.1480271 Vali Loss: 0.1368049 Test Loss: 0.1594414\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.1337425\n",
      "\tspeed: 0.2157s/iter; left time: 446.2245s\n",
      "\titers: 200, epoch: 14 | loss: 0.1519378\n",
      "\tspeed: 0.0520s/iter; left time: 102.3328s\n",
      "\titers: 300, epoch: 14 | loss: 0.1384474\n",
      "\tspeed: 0.0519s/iter; left time: 97.0266s\n",
      "\titers: 400, epoch: 14 | loss: 0.1412669\n",
      "\tspeed: 0.0518s/iter; left time: 91.6506s\n",
      "\titers: 500, epoch: 14 | loss: 0.1538415\n",
      "\tspeed: 0.0519s/iter; left time: 86.5994s\n",
      "\titers: 600, epoch: 14 | loss: 0.1391198\n",
      "\tspeed: 0.0520s/iter; left time: 81.5433s\n",
      "\titers: 700, epoch: 14 | loss: 0.1154895\n",
      "\tspeed: 0.0519s/iter; left time: 76.2993s\n",
      "\titers: 800, epoch: 14 | loss: 0.1464256\n",
      "\tspeed: 0.0519s/iter; left time: 71.0257s\n",
      "\titers: 900, epoch: 14 | loss: 0.1358327\n",
      "\tspeed: 0.0519s/iter; left time: 65.9238s\n",
      "\titers: 1000, epoch: 14 | loss: 0.1338659\n",
      "\tspeed: 0.0522s/iter; left time: 61.0735s\n",
      "Epoch: 14 cost time: 56.3142876625061\n",
      "Backbone Epoch: 9, Steps: 1084 | Train Loss: 0.1476609 Vali Loss: 0.1367255 Test Loss: 0.1594084\n",
      "Validation loss decreased (0.136779 --> 0.136726).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 15 | loss: 0.1701052\n",
      "\tspeed: 0.2175s/iter; left time: 214.2474s\n",
      "\titers: 200, epoch: 15 | loss: 0.1693144\n",
      "\tspeed: 0.0521s/iter; left time: 46.1312s\n",
      "\titers: 300, epoch: 15 | loss: 0.1532812\n",
      "\tspeed: 0.0519s/iter; left time: 40.7181s\n",
      "\titers: 400, epoch: 15 | loss: 0.1399172\n",
      "\tspeed: 0.0519s/iter; left time: 35.5433s\n",
      "\titers: 500, epoch: 15 | loss: 0.1407431\n",
      "\tspeed: 0.0519s/iter; left time: 30.3516s\n",
      "\titers: 600, epoch: 15 | loss: 0.1408566\n",
      "\tspeed: 0.0520s/iter; left time: 25.2261s\n",
      "\titers: 700, epoch: 15 | loss: 0.1274686\n",
      "\tspeed: 0.0519s/iter; left time: 19.9695s\n",
      "\titers: 800, epoch: 15 | loss: 0.1437313\n",
      "\tspeed: 0.0519s/iter; left time: 14.7954s\n",
      "\titers: 900, epoch: 15 | loss: 0.1518637\n",
      "\tspeed: 0.0519s/iter; left time: 9.6084s\n",
      "\titers: 1000, epoch: 15 | loss: 0.1843496\n",
      "\tspeed: 0.0519s/iter; left time: 4.4132s\n",
      "Epoch: 15 cost time: 56.31592273712158\n",
      "Backbone Epoch: 10, Steps: 1084 | Train Loss: 0.1474903 Vali Loss: 0.1366869 Test Loss: 0.1595571\n",
      "Validation loss decreased (0.136726 --> 0.136687).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 9.765625e-07\n",
      "Average time per pretrain epoch: 22.96944s\n",
      "Average time per joint train epoch: 68.41285s\n",
      "test 4925\n",
      "mse:0.15944479405879974, mae:0.262300580739975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1594448, 0.26230058)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_setting_name(configs):\n",
    "    setting = '{}_{}_pl{}_un{}_dc{}_lr({})_splr({})_sjlr({})_tff{}_dset({})_tepoch{}_tnorm{}_snorm{}'.format(\n",
    "        configs.data_path[:-4],\n",
    "        configs.model_name,\n",
    "        configs.pred_len,\n",
    "        configs.use_norm,\n",
    "        configs.decomp_type,\n",
    "        configs.learning_rate,\n",
    "        configs.station_pre_lr,\n",
    "        configs.station_joint_lr,\n",
    "        configs.t_ff,\n",
    "        configs.alpha if configs.decomp_type == 'ema' else configs.kernel_size,\n",
    "        configs.twice_epoch,\n",
    "        configs.t_norm,\n",
    "        configs.s_norm,\n",
    "    )\n",
    "    return setting\n",
    "\n",
    "setting = make_setting_name(configs)\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electricity_iTransformer_pl336_unlt_dcsma_lr(0.001)_splr(0.0005)_sjlr(0.0005)_tff64_dset(25)_tepoch3_tnormTrue_snormFalse'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.3804953098297119, mae:0.4029838442802429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3804953, 0.40298384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 가능한 총 파라미터 수 계산\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in norm.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"모델의 총 학습 가능 파라미터 수: {trainable_params:,} 개\")\n",
    "\n",
    "# 파라미터 저장을 위한 메모리 크기 계산 (기본 float32 기준)\n",
    "# 1 float32 = 4 bytes\n",
    "param_size_bytes = trainable_params * 4\n",
    "param_size_mb = param_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"모델 파라미터의 예상 메모리 크기: {param_size_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7201\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.7581769\n",
      "\tspeed: 0.0053s/iter; left time: 17.3979s\n",
      "\titers: 200, epoch: 1 | loss: 0.9436355\n",
      "\tspeed: 0.0053s/iter; left time: 16.8870s\n",
      "Epoch: 1 cost time: 1.1876552104949951\n",
      "Station Epoch: 1, Steps: 225 | Train Loss: 0.8619922 Vali Loss: 0.7346204 Test Loss: 0.3525207\n",
      "Validation loss decreased (inf --> 0.734620).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 2 | loss: 0.6560441\n",
      "\tspeed: 0.0111s/iter; left time: 33.9457s\n",
      "\titers: 200, epoch: 2 | loss: 0.6961247\n",
      "\tspeed: 0.0054s/iter; left time: 15.8585s\n",
      "Epoch: 2 cost time: 1.2024035453796387\n",
      "Station Epoch: 2, Steps: 225 | Train Loss: 0.8526279 Vali Loss: 0.7229651 Test Loss: 0.3511476\n",
      "Validation loss decreased (0.734620 --> 0.722965).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "\titers: 100, epoch: 3 | loss: 0.7075656\n",
      "\tspeed: 0.0114s/iter; left time: 32.1812s\n",
      "\titers: 200, epoch: 3 | loss: 0.8973257\n",
      "\tspeed: 0.0052s/iter; left time: 14.1843s\n",
      "Epoch: 3 cost time: 1.1764435768127441\n",
      "Station Epoch: 3, Steps: 225 | Train Loss: 0.8455808 Vali Loss: 0.7188644 Test Loss: 0.3504516\n",
      "Validation loss decreased (0.722965 --> 0.718864).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "\titers: 100, epoch: 4 | loss: 0.6242340\n",
      "\tspeed: 0.0109s/iter; left time: 28.4380s\n",
      "\titers: 200, epoch: 4 | loss: 1.3776972\n",
      "\tspeed: 0.0053s/iter; left time: 13.3632s\n",
      "Epoch: 4 cost time: 1.1639304161071777\n",
      "Station Epoch: 4, Steps: 225 | Train Loss: 0.8414394 Vali Loss: 0.7157307 Test Loss: 0.3500996\n",
      "Validation loss decreased (0.718864 --> 0.715731).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "\titers: 100, epoch: 5 | loss: 0.7854842\n",
      "\tspeed: 0.0116s/iter; left time: 27.4570s\n",
      "\titers: 200, epoch: 5 | loss: 1.1309911\n",
      "\tspeed: 0.0054s/iter; left time: 12.3637s\n",
      "Epoch: 5 cost time: 1.2131388187408447\n",
      "Station Epoch: 5, Steps: 225 | Train Loss: 0.8400683 Vali Loss: 0.7150485 Test Loss: 0.3499247\n",
      "Validation loss decreased (0.715731 --> 0.715048).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 1.0100662\n",
      "\tspeed: 0.0240s/iter; left time: 51.6257s\n",
      "\titers: 200, epoch: 6 | loss: 0.6283067\n",
      "\tspeed: 0.0176s/iter; left time: 36.1417s\n",
      "Epoch: 6 cost time: 4.017696380615234\n",
      "Backbone Epoch: 1, Steps: 225 | Train Loss: 0.9767400 Vali Loss: 0.7395227 Test Loss: 0.4435011\n",
      "Validation loss decreased (inf --> 0.739523).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 7 | loss: 1.0917602\n",
      "\tspeed: 0.0322s/iter; left time: 62.0980s\n",
      "\titers: 200, epoch: 7 | loss: 0.6679821\n",
      "\tspeed: 0.0180s/iter; left time: 32.8762s\n",
      "Epoch: 7 cost time: 4.068219184875488\n",
      "Backbone Epoch: 2, Steps: 225 | Train Loss: 0.9089204 Vali Loss: 0.7322854 Test Loss: 0.4288523\n",
      "Validation loss decreased (0.739523 --> 0.732285).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.7607693\n",
      "\tspeed: 0.0330s/iter; left time: 56.2099s\n",
      "\titers: 200, epoch: 8 | loss: 1.1101314\n",
      "\tspeed: 0.0187s/iter; left time: 29.9483s\n",
      "Epoch: 8 cost time: 4.216555118560791\n",
      "Backbone Epoch: 3, Steps: 225 | Train Loss: 0.8916649 Vali Loss: 0.7187335 Test Loss: 0.3999175\n",
      "Validation loss decreased (0.732285 --> 0.718734).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 9 | loss: 1.0895000\n",
      "\tspeed: 0.0313s/iter; left time: 46.2238s\n",
      "\titers: 200, epoch: 9 | loss: 0.6937146\n",
      "\tspeed: 0.0177s/iter; left time: 24.2971s\n",
      "Epoch: 9 cost time: 3.9259064197540283\n",
      "Backbone Epoch: 4, Steps: 225 | Train Loss: 0.8624049 Vali Loss: 0.7083221 Test Loss: 0.3810974\n",
      "Validation loss decreased (0.718734 --> 0.708322).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.8454124\n",
      "\tspeed: 0.0321s/iter; left time: 40.2056s\n",
      "\titers: 200, epoch: 10 | loss: 0.8007569\n",
      "\tspeed: 0.0175s/iter; left time: 20.1722s\n",
      "Epoch: 10 cost time: 4.020641326904297\n",
      "Backbone Epoch: 5, Steps: 225 | Train Loss: 0.8317907 Vali Loss: 0.7071365 Test Loss: 0.3729743\n",
      "Validation loss decreased (0.708322 --> 0.707137).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 11 | loss: 1.2636704\n",
      "\tspeed: 0.0307s/iter; left time: 31.5242s\n",
      "\titers: 200, epoch: 11 | loss: 0.5371707\n",
      "\tspeed: 0.0172s/iter; left time: 15.8832s\n",
      "Epoch: 11 cost time: 3.847550392150879\n",
      "Backbone Epoch: 6, Steps: 225 | Train Loss: 0.8103570 Vali Loss: 0.7118289 Test Loss: 0.3720203\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.8765784\n",
      "\tspeed: 0.0315s/iter; left time: 25.2184s\n",
      "\titers: 200, epoch: 12 | loss: 0.5144888\n",
      "\tspeed: 0.0172s/iter; left time: 12.0404s\n",
      "Epoch: 12 cost time: 3.8610472679138184\n",
      "Backbone Epoch: 7, Steps: 225 | Train Loss: 0.7994788 Vali Loss: 0.7165598 Test Loss: 0.3724926\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.8843291\n",
      "\tspeed: 0.0307s/iter; left time: 17.6630s\n",
      "\titers: 200, epoch: 13 | loss: 0.7674705\n",
      "\tspeed: 0.0171s/iter; left time: 8.1437s\n",
      "Epoch: 13 cost time: 3.8360366821289062\n",
      "Backbone Epoch: 8, Steps: 225 | Train Loss: 0.7953129 Vali Loss: 0.7174545 Test Loss: 0.3723657\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "Average time per pretrain epoch: 1.66244s\n",
      "Average time per joint train epoch: 4.94965s\n",
      "test 2161\n",
      "mse:0.3713555634021759, mae:0.43030044436454773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37135556, 0.43030044)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.root_path='datasets'\n",
    "configs.data='custom'\n",
    "configs.data_path='electricity.csv'\n",
    "\n",
    "configs.model_name=\"iTransformer\"\n",
    "if configs.model_name==\"iTransformer\":\n",
    "    configs.d_model=128\n",
    "    configs.d_ff=128\n",
    "    configs.factor=1\n",
    "elif configs.model_name in [\"Autoformer\", \"FEDformer\"]:\n",
    "    configs.d_model=512\n",
    "    configs.d_ff=2048\n",
    "    configs.factor=3\n",
    "    \n",
    "configs.learning_rate=0.0001\n",
    "configs.seq_len=720\n",
    "configs.label_len=168\n",
    "configs.pred_len=720\n",
    "configs.enc_in=7\n",
    "configs.dec_in=7\n",
    "configs.c_out=7\n",
    "\n",
    "configs.use_norm='lt'\n",
    "configs.station_pre_lr=0.00001\n",
    "configs.station_joint_lr=0.00001\n",
    "configs.t_ff=64\n",
    "configs.alpha=0.2\n",
    "configs.twice_epoch=3\n",
    "configs.use_mlp=True\n",
    "configs.t_norm=True\n",
    "configs.s_norm=False\n",
    "configs.affine=True\n",
    "\n",
    "decomp = ema_decomp(configs.alpha)\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_pre_lr)\n",
    "\n",
    "setting = 'test'\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 결과 포맷팅\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('result.csv')\n",
    "result_list = []\n",
    "for i in range(len(df)):\n",
    "    settings = df.Setting[i].split('_')\n",
    "    dataset = settings[1]\n",
    "    model_name = settings[4]\n",
    "    seq_len = settings[7][2:]\n",
    "    pred_len = settings[9][2:]\n",
    "    trend_norm = settings[-4][2:]\n",
    "    station_pre_lr = settings[-3][3:]\n",
    "    use_mlp = settings[-2][2:]\n",
    "    mse = df.MSE[i]\n",
    "    mae = df.MAE[i]\n",
    "    \n",
    "    result_list.append([dataset, model_name, seq_len, pred_len, trend_norm, station_pre_lr, use_mlp, mse, mae])\n",
    "df_result = pd.DataFrame(result_list, columns=['Dataset', 'Model', 'Seq_Len', 'Pred_Len', 'Trend_Norm', 'Station_LR', 'Use_MLP', 'MSE', 'MAE'])\n",
    "df_result.to_csv('summary_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7585\n"
     ]
    }
   ],
   "source": [
    "data_set, data_loader = data_provider(configs, flag='train')\n",
    "\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = next(iter(data_loader))\n",
    "\n",
    "decomp = envelope_decomp(25).to(device)\n",
    "seasonal_x, trend_x = decomp(batch_y)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(batch_y[0, :, -1].cpu().numpy(), label='Original Series')\n",
    "plt.plot(trend_x[0, :, -1].cpu().numpy(), label='Extracted Trend', linestyle='--')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(seasonal_x[0, :, -1].cpu().numpy(), label='Extracted Seasonal')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.savefig('decomposition_example.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_x, batch_y, batch_x_mark, batch_y_mark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdata_loader\u001b[49m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# visualize column 0~9\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = data_provider(configs, flag='train')[1]\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = next(iter(train_loader))\n",
    "\n",
    "# visualize column 0~9\n",
    "for col in range(10):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(batch_y[0, :, col].cpu().numpy(), label='Original Series')\n",
    "    seasonal_x, trend_x = decomp(batch_y)\n",
    "    plt.plot(trend_x[0, :, col].cpu().numpy(), label='Extracted Trend', linestyle='--')\n",
    "    plt.plot(seasonal_x[0, :, col].cpu().numpy(), label='Extracted Seasonal', linestyle=':')\n",
    "    plt.title(f'Column {col} Decomposition')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'decomposition_column_{col}.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
