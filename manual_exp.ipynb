{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from utils.metrics import metric\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from models import DLinear, iTransformer, Autoformer, FEDformer, Linear\n",
    "from normalizers import SAN, DDN, RevIN, LightTrend, NoNorm, MultiTrend\n",
    "from layers.decomposition import series_decomp, ema_decomp, envelope_decomp\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    task_name = \"long_term_forecast\",\n",
    "    model_name = \"iTransformer\", \n",
    "    \n",
    "    # Task\n",
    "    seq_len=720,\n",
    "    label_len=168,\n",
    "    pred_len=720,\n",
    "    enc_in=862,\n",
    "    dec_in=862,\n",
    "    c_out=862,\n",
    "    features='M',\n",
    "    freq='h',\n",
    "    \n",
    "    # Data\n",
    "    root_path='datasets/ETT-small',\n",
    "    data='ETTm1',\n",
    "    data_path='ETTm1.csv',\n",
    "    target='OT',\n",
    "    batch_size=32,\n",
    "    \n",
    "    # Basic Model Settings\n",
    "    checkpoints='./manual_exp/checkpoints/',\n",
    "    dropout=0.1,\n",
    "    embed='timeF',\n",
    "    output_attention=False,\n",
    "    activation='gelu',\n",
    "    moving_avg=25,\n",
    "    num_kernels=6,\n",
    "    individual=False,\n",
    "    learning_rate=0.0001,\n",
    "    num_workers=5,\n",
    "    n_heads=8,\n",
    "    patience=3,\n",
    "    train_epochs=10,\n",
    "    use_amp=False,\n",
    "    lradj='type1',\n",
    "    embed_type=0,\n",
    "    # e_layers=2,\n",
    "    # factor=3,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    \n",
    "    # iTransformer\n",
    "    # d_model=128,\n",
    "    # d_ff=128,\n",
    "    # d_model=512,\n",
    "    # d_ff=512,\n",
    "    class_strategy='projection',\n",
    "    \n",
    "    # Autoformer & FEDformer\n",
    "    factor=3,\n",
    "    e_layers=2,\n",
    "    d_layers=1,\n",
    "    \n",
    "    # Normalizer\n",
    "    use_norm='lt',\n",
    "    decomp_type='sma',\n",
    "    norm_variant='original',\n",
    "    station_type='adaptive',\n",
    "    \n",
    "    down_ratio=2,\n",
    "    kernel_size=25,\n",
    "    kernel_len=25,\n",
    "    kernel_lens=[25, 97],\n",
    "    station_joint_lr=0.0001,\n",
    "    station_pre_lr=0.0001,\n",
    "    t_ff=128,\n",
    "    alpha=0.04,\n",
    "    twice_epoch=3,\n",
    "    use_mlp=True,\n",
    "    t_norm=True,\n",
    "    s_norm=False,\n",
    "    affine=True,\n",
    "    \n",
    "    period_len=24,\n",
    "    pre_epoch=5,\n",
    "    j=1,\n",
    "    learnable=False,\n",
    "    wavelet='coif3',\n",
    "    dr=0.05,\n",
    "    hkernel_len=5,\n",
    "    pd_ff=128,\n",
    "    pd_model=128,\n",
    "    pe_layers=0,\n",
    "    reinit_mean_pretrain=False,\n",
    "    reinit_std_pretrain=False,\n",
    ")\n",
    "model_dict={\n",
    "    'DLinear': DLinear,\n",
    "    'iTransformer': iTransformer,\n",
    "    'Autoformer': Autoformer,\n",
    "    'FEDformer': FEDformer,\n",
    "    'Linear': Linear,\n",
    "}\n",
    "linear_models = ['DLinear', 'Linear']\n",
    "norm_dict = {\n",
    "    'none': NoNorm,\n",
    "    'revin': RevIN,\n",
    "    'san': SAN,\n",
    "    'ddn': DDN,\n",
    "    'lt': LightTrend,\n",
    "    'mt': MultiTrend,\n",
    "}\n",
    "\n",
    "if configs.decomp_type == 'sma':\n",
    "    decomp = series_decomp(configs.kernel_len)\n",
    "elif configs.decomp_type == 'ema':\n",
    "    decomp = ema_decomp(configs.alpha)\n",
    "elif configs.decomp_type == 'envelope':\n",
    "    decomp = envelope_decomp(configs.kernel_len)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "norm_criterion = nn.MSELoss()\n",
    "# norm_criterion = nn.L1Loss()\n",
    "\n",
    "# SAN\n",
    "def san_loss(y, statistics_pred):\n",
    "    bs, len, dim = y.shape\n",
    "    y = y.reshape(bs, -1, configs.period_len, dim)\n",
    "    mean = torch.mean(y, dim=2)\n",
    "    std = torch.std(y, dim=2)\n",
    "    station_ture = torch.cat([mean, std], dim=-1)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# DDN\n",
    "def ddn_loss(y, statistics_pred):\n",
    "    _, (mean, std) = norm.norm(y.transpose(-1, -2), False)\n",
    "    station_ture = torch.cat([mean, std], dim=1).transpose(-1, -2)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# TREAD\n",
    "def tread_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred[-1]\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "# LightTrend\n",
    "def lt_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = norm_criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "station_loss_dict = {\n",
    "    'none': None,\n",
    "    'revin': None,\n",
    "    'san': san_loss,\n",
    "    'ddn': ddn_loss,\n",
    "    'san_exp': san_loss,\n",
    "    'ddn_exp': ddn_loss,\n",
    "    'lt': lt_loss,\n",
    "    'mt': lt_loss,\n",
    "}\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "\n",
    "# [pre train, pre epoch, joint train, join epoch]\n",
    "station_setting_dict = {\n",
    "    'none': [0, 0, 0, 0],\n",
    "    'revin': [0, 0, 0, 0],\n",
    "    'san': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'lt': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'mt': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'san_exp': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn_exp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "}\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_pre_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion, epoch):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "\n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float()\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "            total_loss.append(loss.cpu().item())\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    norm.train()\n",
    "    return total_loss\n",
    "\n",
    "def _get_data(flag):\n",
    "    data_set, data_loader = data_provider(configs, flag)\n",
    "    return data_set, data_loader\n",
    "\n",
    "def train(setting):\n",
    "    train_data, train_loader = _get_data(flag='train')\n",
    "    vali_data, vali_loader = _get_data(flag='val')\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    path = os.path.join(configs.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    path_station = './station/' + '{}_s{}_p{}'.format(configs.use_norm, configs.data,\n",
    "                                                        configs.seq_len, configs.pred_len)\n",
    "    if not os.path.exists(path_station):\n",
    "        os.makedirs(path_station)\n",
    "\n",
    "    time_now = time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "    early_stopping_station_model = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "\n",
    "    if configs.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    ### NEW ###\n",
    "    # 재초기화를 위한 헬퍼 함수 정의\n",
    "    def reinit_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters() # PyTorch 기본값으로 가중치 초기화\n",
    "    ### END NEW ###\n",
    "\n",
    "    time_per_epoch = []\n",
    "    for epoch in range(configs.train_epochs + station_setting[1]):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        epoch_start = time()\n",
    "        # Load best station model after pretraining\n",
    "        if station_setting[0] > 0 and epoch == station_setting[1]:\n",
    "            best_model_path = path_station + '/' + 'checkpoint.pth'\n",
    "            norm.load_state_dict(torch.load(best_model_path))\n",
    "            print('loading pretrained adaptive station model')\n",
    "            \n",
    "            ### NEW ###\n",
    "            # --- Motivating Experiment 3.2: 학습 분리 효과 검증 ---\n",
    "            # 'DDN + w/o trend pretrain' 실험\n",
    "            if getattr(configs, 'reinit_mean_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing MEAN predictor weights !!!\")\n",
    "                norm.mlp.m_project.apply(reinit_weights)\n",
    "                norm.mlp.mean_proj.apply(reinit_weights)\n",
    "                norm.mlp.m_concat.apply(reinit_weights)\n",
    "                norm.mlp.mean_ffn.apply(reinit_weights)\n",
    "                norm.mlp.mean_pred.apply(reinit_weights)\n",
    "                print(\"!!! Mean predictor re-initialization complete. !!!\")\n",
    "\n",
    "            # 'DDN + w/o seasonal pretrain' (std) 실험\n",
    "            if getattr(configs, 'reinit_std_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing STD predictor weights !!!\")\n",
    "                norm.mlp.s_project.apply(reinit_weights)\n",
    "                norm.mlp.std_proj.apply(reinit_weights)\n",
    "                norm.mlp.s_concat.apply(reinit_weights)\n",
    "                norm.mlp.std_ffn.apply(reinit_weights)\n",
    "                norm.mlp.std_pred.apply(reinit_weights)\n",
    "                print(\"!!! Std predictor re-initialization complete. !!!\")\n",
    "            \n",
    "            # 'DDN + w/o pretrain' 실험은 station_setting[1] = 0 으로 설정하고\n",
    "            # 이 if 블록에 들어오지 않게 함으로써 자연스럽게 구현됩니다.\n",
    "            ### END NEW ###\n",
    "        \n",
    "        # Add station parameters to model optim after pretraining and delay epochs for joint training\n",
    "        if station_setting[2] > 0 and station_setting[3] == epoch - station_setting[1] + 1:\n",
    "            # lr = model_optim.param_groups[0]['lr']\n",
    "            lr = configs.station_joint_lr\n",
    "            # lr = norm_optim.param_groups[0]['lr']\n",
    "            model_optim.add_param_group({'params': norm.parameters(), 'lr': lr})\n",
    "        \n",
    "        model.train()\n",
    "        norm.train()\n",
    "        epoch_time = time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            \n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                        f_dim = -1 if configs.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                    f_dim = -1 if configs.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                    if configs.features == 'MS':\n",
    "                        statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                        \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time() - time_now) / iter_count\n",
    "                left_time = speed * (\n",
    "                        (configs.train_epochs + station_setting[1] - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time()\n",
    "            if configs.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                # two-stage training schema\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    norm_optim.step()\n",
    "                else:\n",
    "                    model_optim.step()\n",
    "                model_optim.zero_grad()\n",
    "                norm_optim.zero_grad()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion, epoch)\n",
    "        test_loss = vali(test_data, test_loader, criterion, epoch)\n",
    "\n",
    "        if epoch + 1 <= station_setting[1]:\n",
    "            print(\n",
    "                \"Station Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping_station_model(vali_loss, norm, path_station)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1, configs, configs.station_pre_lr)\n",
    "        else:\n",
    "            print(\n",
    "                \"Backbone Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1 - station_setting[1], train_steps, train_loss, vali_loss, test_loss))\n",
    "            # if: joint training, else: only model training\n",
    "            if station_setting[2] > 0 and station_setting[3] <= epoch - station_setting[1]:\n",
    "                early_stopping(vali_loss, model, path, norm, path_station)\n",
    "            else:\n",
    "                early_stopping(vali_loss, model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(model_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.learning_rate)\n",
    "            # adjust_learning_rate(norm_optim, epoch + 1 - station_setting[1], configs,\n",
    "            #                         configs.station_pre_lr)\n",
    "        time_per_epoch.append(time() - epoch_start)\n",
    "    print(\"Average time per pretrain epoch: {:.5f}s\".format(np.mean(time_per_epoch[:5])))\n",
    "    print(\"Average time per joint train epoch: {:.5f}s\".format(np.mean(time_per_epoch[5:])))\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if station_setting[2] > 0:\n",
    "        norm.load_state_dict(torch.load(path_station + '/' + 'checkpoint.pth'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(setting, test=0):\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    if test:\n",
    "        print('loading model')\n",
    "        model.load_state_dict(torch.load(os.path.join('./manual_exp/checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    inputx = []\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            input_x = batch_x\n",
    "\n",
    "            # normalize\n",
    "            input_noNorm = batch_x.clone()\n",
    "            _, trend_noNorm = decomp(input_noNorm)\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            input_withNorm = batch_x\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder x\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "            dec_label = batch_x[:, -configs.label_len:, :]\n",
    "            dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "            # encoder - decoder\n",
    "            if configs.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            else:\n",
    "                if configs.model_name in linear_models:\n",
    "                    outputs = model(batch_x)\n",
    "                else:\n",
    "                    if configs.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "            f_dim = -1 if configs.features == 'MS' else 0\n",
    "            outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "            if configs.features == 'MS':\n",
    "                statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                \n",
    "            # de-normalize\n",
    "            output_noDenorm = outputs.clone()\n",
    "            outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "            output_withDenorm = outputs\n",
    "            \n",
    "            batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "            _, trend_true = decomp(batch_y)\n",
    "            trend_true = trend_true.detach().cpu().numpy()\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "            pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "            true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "            \n",
    "            # LightTrend가 예측한 추세 시각화\n",
    "            output_station_noDenorm = statistics_pred.clone() if configs.use_norm == 'lt' else None\n",
    "\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "            inputx.append(batch_x.detach().cpu().numpy())\n",
    "            if i % 20 == 0:\n",
    "                x = input_x.detach().cpu().numpy()\n",
    "                gt = np.concatenate((x[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                pd = np.concatenate((x[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "            \n",
    "            # visualize input and output with/without normalization/denormalization\n",
    "            if i == 0:\n",
    "                i_n = input_noNorm.detach().cpu().numpy()\n",
    "                t_n = trend_noNorm.detach().cpu().numpy()\n",
    "                i_w = input_withNorm.detach().cpu().numpy()\n",
    "                o_n = output_noDenorm.detach().cpu().numpy()\n",
    "                o_w = output_withDenorm.detach().cpu().numpy()\n",
    "                os_n = output_station_noDenorm.detach().cpu().numpy() if configs.use_norm == 'lt' else None\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(i_n[0, :, -1], label='Original')\n",
    "                plt.plot(t_n[0, :, -1], label='Trend', color='black', linestyle='--')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(i_w[0, :, -1], label='Normalized')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_input.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='Ground Truth', color='black', linewidth=2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(0, label='Before Norm(Model Output)')\n",
    "                plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3)\n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(o_n[0, :, -1], label='Before Norm(Model Output)')\n",
    "                # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_output.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='True')\n",
    "                plt.plot(o_w[0, :, -1], label='Model Output')\n",
    "                if configs.use_norm == 'lt':\n",
    "                    plt.plot(os_n[0, :, -1], label='Station Output')\n",
    "                plt.legend()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(trend_true[0, :, -1], label='Trend True')\n",
    "                if configs.use_norm == 'lt':\n",
    "                    plt.plot(os_n[0, :, -1], label='Trend Predicted')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(folder_path, 'station_output.pdf'))\n",
    "\n",
    "    # if configs.test_flop:\n",
    "    #     test_params_flop((batch_x.shape[1], batch_x.shape[2]))\n",
    "    #     exit()\n",
    "    preds = np.array(preds, dtype=object)\n",
    "    trues = np.array(trues, dtype=object)\n",
    "    # inputx = np.array(inputx)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    # inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "    # result save\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "    print('mse:{}, mae:{}'.format(mse, mae))\n",
    "    f = open(\"result.txt\", 'a')\n",
    "    f.write(setting + \"  \\n\")\n",
    "    f.write('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 33745\n",
      "val 10801\n",
      "test 10801\n",
      "\titers: 100, epoch: 1 | loss: 0.3899218\n",
      "\tspeed: 0.0075s/iter; left time: 118.1039s\n",
      "\titers: 200, epoch: 1 | loss: 0.4138595\n",
      "\tspeed: 0.0076s/iter; left time: 117.9882s\n",
      "\titers: 300, epoch: 1 | loss: 0.4441792\n",
      "\tspeed: 0.0068s/iter; left time: 105.3269s\n",
      "\titers: 400, epoch: 1 | loss: 0.4122805\n",
      "\tspeed: 0.0068s/iter; left time: 105.3080s\n",
      "\titers: 500, epoch: 1 | loss: 0.3818064\n",
      "\tspeed: 0.0067s/iter; left time: 102.9815s\n",
      "\titers: 600, epoch: 1 | loss: 0.4641036\n",
      "\tspeed: 0.0071s/iter; left time: 108.0461s\n",
      "\titers: 700, epoch: 1 | loss: 0.4076684\n",
      "\tspeed: 0.0072s/iter; left time: 109.4041s\n",
      "\titers: 800, epoch: 1 | loss: 0.3958711\n",
      "\tspeed: 0.0069s/iter; left time: 104.0770s\n",
      "\titers: 900, epoch: 1 | loss: 0.3558349\n",
      "\tspeed: 0.0071s/iter; left time: 106.1611s\n",
      "\titers: 1000, epoch: 1 | loss: 0.4075990\n",
      "\tspeed: 0.0069s/iter; left time: 102.9273s\n",
      "Epoch: 1 cost time: 7.432176828384399\n",
      "Station Epoch: 1, Steps: 1054 | Train Loss: 0.4040911 Vali Loss: 0.8858407 Test Loss: 0.3511732\n",
      "Validation loss decreased (inf --> 0.885841).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4199495\n",
      "\tspeed: 0.0379s/iter; left time: 555.3300s\n",
      "\titers: 200, epoch: 2 | loss: 0.3341395\n",
      "\tspeed: 0.0067s/iter; left time: 96.9354s\n",
      "\titers: 300, epoch: 2 | loss: 0.3779946\n",
      "\tspeed: 0.0072s/iter; left time: 104.6773s\n",
      "\titers: 400, epoch: 2 | loss: 0.3468982\n",
      "\tspeed: 0.0071s/iter; left time: 101.9209s\n",
      "\titers: 500, epoch: 2 | loss: 0.3722397\n",
      "\tspeed: 0.0069s/iter; left time: 97.7593s\n",
      "\titers: 600, epoch: 2 | loss: 0.4644545\n",
      "\tspeed: 0.0072s/iter; left time: 101.5628s\n",
      "\titers: 700, epoch: 2 | loss: 0.3539633\n",
      "\tspeed: 0.0075s/iter; left time: 104.7842s\n",
      "\titers: 800, epoch: 2 | loss: 0.4092935\n",
      "\tspeed: 0.0068s/iter; left time: 95.1789s\n",
      "\titers: 900, epoch: 2 | loss: 0.3755296\n",
      "\tspeed: 0.0069s/iter; left time: 95.1508s\n",
      "\titers: 1000, epoch: 2 | loss: 0.4179360\n",
      "\tspeed: 0.0067s/iter; left time: 92.0551s\n",
      "Epoch: 2 cost time: 7.33370041847229\n",
      "Station Epoch: 2, Steps: 1054 | Train Loss: 0.3860169 Vali Loss: 0.8766490 Test Loss: 0.3432958\n",
      "Validation loss decreased (0.885841 --> 0.876649).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3218005\n",
      "\tspeed: 0.0350s/iter; left time: 476.3975s\n",
      "\titers: 200, epoch: 3 | loss: 0.3369266\n",
      "\tspeed: 0.0072s/iter; left time: 97.3499s\n",
      "\titers: 300, epoch: 3 | loss: 0.4700747\n",
      "\tspeed: 0.0068s/iter; left time: 91.7223s\n",
      "\titers: 400, epoch: 3 | loss: 0.3710179\n",
      "\tspeed: 0.0068s/iter; left time: 90.5374s\n",
      "\titers: 500, epoch: 3 | loss: 0.3806632\n",
      "\tspeed: 0.0069s/iter; left time: 91.5587s\n",
      "\titers: 600, epoch: 3 | loss: 0.3946389\n",
      "\tspeed: 0.0071s/iter; left time: 92.5252s\n",
      "\titers: 700, epoch: 3 | loss: 0.3455793\n",
      "\tspeed: 0.0069s/iter; left time: 89.7005s\n",
      "\titers: 800, epoch: 3 | loss: 0.4835787\n",
      "\tspeed: 0.0068s/iter; left time: 87.4408s\n",
      "\titers: 900, epoch: 3 | loss: 0.3499024\n",
      "\tspeed: 0.0069s/iter; left time: 88.9420s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3248529\n",
      "\tspeed: 0.0071s/iter; left time: 89.9132s\n",
      "Epoch: 3 cost time: 7.269305467605591\n",
      "Station Epoch: 3, Steps: 1054 | Train Loss: 0.3818718 Vali Loss: 0.8754155 Test Loss: 0.3418950\n",
      "Validation loss decreased (0.876649 --> 0.875415).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.4128535\n",
      "\tspeed: 0.0399s/iter; left time: 500.1295s\n",
      "\titers: 200, epoch: 4 | loss: 0.3537121\n",
      "\tspeed: 0.0074s/iter; left time: 92.1260s\n",
      "\titers: 300, epoch: 4 | loss: 0.3793370\n",
      "\tspeed: 0.0074s/iter; left time: 91.5160s\n",
      "\titers: 400, epoch: 4 | loss: 0.3978840\n",
      "\tspeed: 0.0071s/iter; left time: 87.5048s\n",
      "\titers: 500, epoch: 4 | loss: 0.3059044\n",
      "\tspeed: 0.0073s/iter; left time: 88.6054s\n",
      "\titers: 600, epoch: 4 | loss: 0.3197327\n",
      "\tspeed: 0.0070s/iter; left time: 84.6242s\n",
      "\titers: 700, epoch: 4 | loss: 0.3874789\n",
      "\tspeed: 0.0066s/iter; left time: 78.4370s\n",
      "\titers: 800, epoch: 4 | loss: 0.3481425\n",
      "\tspeed: 0.0070s/iter; left time: 83.3450s\n",
      "\titers: 900, epoch: 4 | loss: 0.3787214\n",
      "\tspeed: 0.0069s/iter; left time: 80.9627s\n",
      "\titers: 1000, epoch: 4 | loss: 0.4073873\n",
      "\tspeed: 0.0071s/iter; left time: 82.8320s\n",
      "Epoch: 4 cost time: 7.435962438583374\n",
      "Station Epoch: 4, Steps: 1054 | Train Loss: 0.3802328 Vali Loss: 0.8737856 Test Loss: 0.3415875\n",
      "Validation loss decreased (0.875415 --> 0.873786).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4027781\n",
      "\tspeed: 0.0403s/iter; left time: 462.9173s\n",
      "\titers: 200, epoch: 5 | loss: 0.2788967\n",
      "\tspeed: 0.0070s/iter; left time: 79.6519s\n",
      "\titers: 300, epoch: 5 | loss: 0.3381358\n",
      "\tspeed: 0.0072s/iter; left time: 80.7730s\n",
      "\titers: 400, epoch: 5 | loss: 0.4221421\n",
      "\tspeed: 0.0070s/iter; left time: 78.7212s\n",
      "\titers: 500, epoch: 5 | loss: 0.2923969\n",
      "\tspeed: 0.0072s/iter; left time: 80.3674s\n",
      "\titers: 600, epoch: 5 | loss: 0.3766043\n",
      "\tspeed: 0.0068s/iter; left time: 74.5358s\n",
      "\titers: 700, epoch: 5 | loss: 0.4511577\n",
      "\tspeed: 0.0067s/iter; left time: 72.6572s\n",
      "\titers: 800, epoch: 5 | loss: 0.3849937\n",
      "\tspeed: 0.0069s/iter; left time: 74.6960s\n",
      "\titers: 900, epoch: 5 | loss: 0.4476757\n",
      "\tspeed: 0.0069s/iter; left time: 74.1772s\n",
      "\titers: 1000, epoch: 5 | loss: 0.5468863\n",
      "\tspeed: 0.0065s/iter; left time: 68.5634s\n",
      "Epoch: 5 cost time: 7.30130934715271\n",
      "Station Epoch: 5, Steps: 1054 | Train Loss: 0.3795219 Vali Loss: 0.8744310 Test Loss: 0.3411588\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 0.4819300\n",
      "\tspeed: 0.4893s/iter; left time: 5108.5078s\n",
      "\titers: 200, epoch: 6 | loss: 0.4206677\n",
      "\tspeed: 0.4040s/iter; left time: 4177.6610s\n",
      "\titers: 300, epoch: 6 | loss: 0.4480784\n",
      "\tspeed: 0.4242s/iter; left time: 4344.3262s\n",
      "\titers: 400, epoch: 6 | loss: 0.4093850\n",
      "\tspeed: 0.4750s/iter; left time: 4817.3479s\n",
      "\titers: 500, epoch: 6 | loss: 0.4263617\n",
      "\tspeed: 0.4002s/iter; left time: 4018.5668s\n",
      "\titers: 600, epoch: 6 | loss: 0.4551816\n",
      "\tspeed: 0.4198s/iter; left time: 4173.4808s\n",
      "\titers: 700, epoch: 6 | loss: 0.4305660\n",
      "\tspeed: 0.4841s/iter; left time: 4764.2057s\n",
      "\titers: 800, epoch: 6 | loss: 0.4354153\n",
      "\tspeed: 0.4200s/iter; left time: 4090.8397s\n",
      "\titers: 900, epoch: 6 | loss: 0.3693458\n",
      "\tspeed: 0.4195s/iter; left time: 4044.1756s\n",
      "\titers: 1000, epoch: 6 | loss: 0.4310647\n",
      "\tspeed: 0.4687s/iter; left time: 4472.2396s\n",
      "Epoch: 6 cost time: 458.6695668697357\n",
      "Backbone Epoch: 1, Steps: 1054 | Train Loss: 0.4598248 Vali Loss: 1.0284312 Test Loss: 0.5022432\n",
      "Validation loss decreased (inf --> 1.028431).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 7 | loss: 0.4441562\n",
      "\tspeed: 2.1351s/iter; left time: 20042.4036s\n",
      "\titers: 200, epoch: 7 | loss: 0.4544561\n",
      "\tspeed: 0.3065s/iter; left time: 2846.3091s\n",
      "\titers: 300, epoch: 7 | loss: 0.3643359\n",
      "\tspeed: 0.3068s/iter; left time: 2818.5116s\n",
      "\titers: 400, epoch: 7 | loss: 0.3719934\n",
      "\tspeed: 0.3244s/iter; left time: 2947.6801s\n",
      "\titers: 500, epoch: 7 | loss: 0.3676990\n",
      "\tspeed: 0.3203s/iter; left time: 2878.8266s\n",
      "\titers: 600, epoch: 7 | loss: 0.4646359\n",
      "\tspeed: 0.3234s/iter; left time: 2874.1733s\n",
      "\titers: 700, epoch: 7 | loss: 0.4551843\n",
      "\tspeed: 0.3192s/iter; left time: 2805.0830s\n",
      "\titers: 800, epoch: 7 | loss: 0.3811979\n",
      "\tspeed: 0.3235s/iter; left time: 2810.2062s\n",
      "\titers: 900, epoch: 7 | loss: 0.4016282\n",
      "\tspeed: 0.3194s/iter; left time: 2742.6900s\n",
      "\titers: 1000, epoch: 7 | loss: 0.4200660\n",
      "\tspeed: 0.3084s/iter; left time: 2617.1717s\n",
      "Epoch: 7 cost time: 328.8566870689392\n",
      "Backbone Epoch: 2, Steps: 1054 | Train Loss: 0.4148432 Vali Loss: 1.0268448 Test Loss: 0.5124550\n",
      "Validation loss decreased (1.028431 --> 1.026845).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3623487\n",
      "\tspeed: 1.9559s/iter; left time: 16298.7953s\n",
      "\titers: 200, epoch: 8 | loss: 0.3478250\n",
      "\tspeed: 0.3245s/iter; left time: 2671.5542s\n",
      "\titers: 300, epoch: 8 | loss: 0.3731596\n",
      "\tspeed: 0.3255s/iter; left time: 2647.3238s\n",
      "\titers: 400, epoch: 8 | loss: 0.3795254\n",
      "\tspeed: 0.3264s/iter; left time: 2622.2489s\n",
      "\titers: 500, epoch: 8 | loss: 0.3596165\n",
      "\tspeed: 0.2866s/iter; left time: 2273.3975s\n",
      "\titers: 600, epoch: 8 | loss: 0.4233031\n",
      "\tspeed: 0.3058s/iter; left time: 2395.4027s\n",
      "\titers: 700, epoch: 8 | loss: 0.3586843\n",
      "\tspeed: 0.3063s/iter; left time: 2368.9000s\n",
      "\titers: 800, epoch: 8 | loss: 0.3474656\n",
      "\tspeed: 0.3227s/iter; left time: 2462.8160s\n",
      "\titers: 900, epoch: 8 | loss: 0.3056276\n",
      "\tspeed: 0.3349s/iter; left time: 2522.4462s\n",
      "\titers: 1000, epoch: 8 | loss: 0.3133298\n",
      "\tspeed: 0.3286s/iter; left time: 2442.3724s\n",
      "Epoch: 8 cost time: 336.5835852622986\n",
      "Backbone Epoch: 3, Steps: 1054 | Train Loss: 0.3706935 Vali Loss: 1.0177500 Test Loss: 0.5060077\n",
      "Validation loss decreased (1.026845 --> 1.017750).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3787563\n",
      "\tspeed: 1.9684s/iter; left time: 14327.7925s\n",
      "\titers: 200, epoch: 9 | loss: 0.3523086\n",
      "\tspeed: 0.3106s/iter; left time: 2229.8349s\n",
      "\titers: 300, epoch: 9 | loss: 0.3811354\n",
      "\tspeed: 0.3446s/iter; left time: 2439.5820s\n",
      "\titers: 400, epoch: 9 | loss: 0.3160516\n",
      "\tspeed: 0.3373s/iter; left time: 2353.8857s\n",
      "\titers: 500, epoch: 9 | loss: 0.3137234\n",
      "\tspeed: 0.3445s/iter; left time: 2369.8387s\n",
      "\titers: 600, epoch: 9 | loss: 0.3296155\n",
      "\tspeed: 0.3444s/iter; left time: 2334.4112s\n",
      "\titers: 700, epoch: 9 | loss: 0.3092355\n",
      "\tspeed: 0.3369s/iter; left time: 2250.4711s\n",
      "\titers: 800, epoch: 9 | loss: 0.3315113\n",
      "\tspeed: 0.3452s/iter; left time: 2271.0212s\n",
      "\titers: 900, epoch: 9 | loss: 0.3207044\n",
      "\tspeed: 0.3388s/iter; left time: 2194.9597s\n",
      "\titers: 1000, epoch: 9 | loss: 0.3539272\n",
      "\tspeed: 0.2744s/iter; left time: 1750.3048s\n",
      "Epoch: 9 cost time: 348.23835468292236\n",
      "Backbone Epoch: 4, Steps: 1054 | Train Loss: 0.3475474 Vali Loss: 1.0091344 Test Loss: 0.5079076\n",
      "Validation loss decreased (1.017750 --> 1.009134).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.3315332\n",
      "\tspeed: 2.2101s/iter; left time: 13757.8131s\n",
      "\titers: 200, epoch: 10 | loss: 0.3274708\n",
      "\tspeed: 0.4825s/iter; left time: 2955.4916s\n",
      "\titers: 300, epoch: 10 | loss: 0.3143724\n",
      "\tspeed: 0.4965s/iter; left time: 2991.5587s\n",
      "\titers: 400, epoch: 10 | loss: 0.3515950\n",
      "\tspeed: 0.4591s/iter; left time: 2720.2346s\n",
      "\titers: 500, epoch: 10 | loss: 0.4117920\n",
      "\tspeed: 0.4596s/iter; left time: 2677.1162s\n",
      "\titers: 600, epoch: 10 | loss: 0.3457589\n",
      "\tspeed: 0.4539s/iter; left time: 2598.4369s\n",
      "\titers: 700, epoch: 10 | loss: 0.3538401\n",
      "\tspeed: 0.4799s/iter; left time: 2699.4072s\n",
      "\titers: 800, epoch: 10 | loss: 0.3047590\n",
      "\tspeed: 0.4917s/iter; left time: 2716.6522s\n",
      "\titers: 900, epoch: 10 | loss: 0.3445291\n",
      "\tspeed: 0.4525s/iter; left time: 2454.5423s\n",
      "\titers: 1000, epoch: 10 | loss: 0.3517663\n",
      "\tspeed: 0.4479s/iter; left time: 2385.1873s\n",
      "Epoch: 10 cost time: 487.9022340774536\n",
      "Backbone Epoch: 5, Steps: 1054 | Train Loss: 0.3374164 Vali Loss: 1.0135097 Test Loss: 0.5066722\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3340237\n",
      "\tspeed: 2.3680s/iter; left time: 12244.7621s\n",
      "\titers: 200, epoch: 11 | loss: 0.3642046\n",
      "\tspeed: 0.3689s/iter; left time: 1870.7519s\n",
      "\titers: 300, epoch: 11 | loss: 0.3486054\n",
      "\tspeed: 0.3543s/iter; left time: 1761.0550s\n",
      "\titers: 400, epoch: 11 | loss: 0.3611681\n",
      "\tspeed: 0.3544s/iter; left time: 1726.0684s\n",
      "\titers: 500, epoch: 11 | loss: 0.2957784\n",
      "\tspeed: 0.3709s/iter; left time: 1769.5468s\n",
      "\titers: 600, epoch: 11 | loss: 0.3830554\n",
      "\tspeed: 0.3621s/iter; left time: 1691.1634s\n",
      "\titers: 700, epoch: 11 | loss: 0.3530616\n",
      "\tspeed: 0.3989s/iter; left time: 1823.4903s\n",
      "\titers: 800, epoch: 11 | loss: 0.3107484\n",
      "\tspeed: 0.4518s/iter; left time: 2020.0583s\n",
      "\titers: 900, epoch: 11 | loss: 0.3065174\n",
      "\tspeed: 0.4853s/iter; left time: 2121.0898s\n",
      "\titers: 1000, epoch: 11 | loss: 0.3131851\n",
      "\tspeed: 0.4281s/iter; left time: 1828.4208s\n",
      "Epoch: 11 cost time: 421.5358233451843\n",
      "Backbone Epoch: 6, Steps: 1054 | Train Loss: 0.3327016 Vali Loss: 1.0212418 Test Loss: 0.5129427\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.3134985\n",
      "\tspeed: 2.6173s/iter; left time: 10775.2219s\n",
      "\titers: 200, epoch: 12 | loss: 0.3859408\n",
      "\tspeed: 0.4522s/iter; left time: 1816.4410s\n",
      "\titers: 300, epoch: 12 | loss: 0.2893648\n",
      "\tspeed: 0.4753s/iter; left time: 1861.7926s\n",
      "\titers: 400, epoch: 12 | loss: 0.3366610\n",
      "\tspeed: 0.4231s/iter; left time: 1615.0432s\n",
      "\titers: 500, epoch: 12 | loss: 0.3221343\n",
      "\tspeed: 0.4622s/iter; left time: 1717.9694s\n",
      "\titers: 600, epoch: 12 | loss: 0.2894456\n",
      "\tspeed: 0.4777s/iter; left time: 1727.7638s\n",
      "\titers: 700, epoch: 12 | loss: 0.2938541\n",
      "\tspeed: 0.3630s/iter; left time: 1276.7778s\n",
      "\titers: 800, epoch: 12 | loss: 0.3951431\n",
      "\tspeed: 0.2834s/iter; left time: 968.3170s\n",
      "\titers: 900, epoch: 12 | loss: 0.3292675\n",
      "\tspeed: 0.3020s/iter; left time: 1001.6744s\n",
      "\titers: 1000, epoch: 12 | loss: 0.3116035\n",
      "\tspeed: 0.3589s/iter; left time: 1154.6235s\n",
      "Epoch: 12 cost time: 420.15886092185974\n",
      "Backbone Epoch: 7, Steps: 1054 | Train Loss: 0.3301911 Vali Loss: 1.0213744 Test Loss: 0.5133140\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "Average time per pretrain epoch: 10.16577s\n",
      "Average time per joint train epoch: 561.62783s\n",
      "test 10801\n",
      "mse:0.5076348185539246, mae:0.492872029542923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5076348, 0.49287203)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_setting_name(configs):\n",
    "    setting = '{}_{}_pl{}_un{}_dc{}_lr({})_splr({})_sjlr({})_tff{}_dset({})_tepoch{}_tnorm{}_snorm{}'.format(\n",
    "        configs.data_path[:-4],\n",
    "        configs.model_name,\n",
    "        configs.pred_len,\n",
    "        configs.use_norm,\n",
    "        configs.decomp_type,\n",
    "        configs.learning_rate,\n",
    "        configs.station_pre_lr,\n",
    "        configs.station_joint_lr,\n",
    "        configs.t_ff,\n",
    "        configs.alpha if configs.decomp_type == 'ema' else configs.kernel_size,\n",
    "        configs.twice_epoch,\n",
    "        configs.t_norm,\n",
    "        configs.s_norm,\n",
    "    )\n",
    "    return setting\n",
    "\n",
    "setting = make_setting_name(configs)\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ETTh2_iTransformer_pl96_unlt_dcsma_lr(0.0001)_splr(1e-05)_sjlr(0.0001)_tff64_dset(25)_tepoch3_tnormTrue_snormFalse'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.3804953098297119, mae:0.4029838442802429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3804953, 0.40298384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 가능한 총 파라미터 수 계산\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in norm.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"모델의 총 학습 가능 파라미터 수: {trainable_params:,} 개\")\n",
    "\n",
    "# 파라미터 저장을 위한 메모리 크기 계산 (기본 float32 기준)\n",
    "# 1 float32 = 4 bytes\n",
    "param_size_bytes = trainable_params * 4\n",
    "param_size_mb = param_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"모델 파라미터의 예상 메모리 크기: {param_size_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7201\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.7581769\n",
      "\tspeed: 0.0053s/iter; left time: 17.3979s\n",
      "\titers: 200, epoch: 1 | loss: 0.9436355\n",
      "\tspeed: 0.0053s/iter; left time: 16.8870s\n",
      "Epoch: 1 cost time: 1.1876552104949951\n",
      "Station Epoch: 1, Steps: 225 | Train Loss: 0.8619922 Vali Loss: 0.7346204 Test Loss: 0.3525207\n",
      "Validation loss decreased (inf --> 0.734620).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 2 | loss: 0.6560441\n",
      "\tspeed: 0.0111s/iter; left time: 33.9457s\n",
      "\titers: 200, epoch: 2 | loss: 0.6961247\n",
      "\tspeed: 0.0054s/iter; left time: 15.8585s\n",
      "Epoch: 2 cost time: 1.2024035453796387\n",
      "Station Epoch: 2, Steps: 225 | Train Loss: 0.8526279 Vali Loss: 0.7229651 Test Loss: 0.3511476\n",
      "Validation loss decreased (0.734620 --> 0.722965).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "\titers: 100, epoch: 3 | loss: 0.7075656\n",
      "\tspeed: 0.0114s/iter; left time: 32.1812s\n",
      "\titers: 200, epoch: 3 | loss: 0.8973257\n",
      "\tspeed: 0.0052s/iter; left time: 14.1843s\n",
      "Epoch: 3 cost time: 1.1764435768127441\n",
      "Station Epoch: 3, Steps: 225 | Train Loss: 0.8455808 Vali Loss: 0.7188644 Test Loss: 0.3504516\n",
      "Validation loss decreased (0.722965 --> 0.718864).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "\titers: 100, epoch: 4 | loss: 0.6242340\n",
      "\tspeed: 0.0109s/iter; left time: 28.4380s\n",
      "\titers: 200, epoch: 4 | loss: 1.3776972\n",
      "\tspeed: 0.0053s/iter; left time: 13.3632s\n",
      "Epoch: 4 cost time: 1.1639304161071777\n",
      "Station Epoch: 4, Steps: 225 | Train Loss: 0.8414394 Vali Loss: 0.7157307 Test Loss: 0.3500996\n",
      "Validation loss decreased (0.718864 --> 0.715731).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "\titers: 100, epoch: 5 | loss: 0.7854842\n",
      "\tspeed: 0.0116s/iter; left time: 27.4570s\n",
      "\titers: 200, epoch: 5 | loss: 1.1309911\n",
      "\tspeed: 0.0054s/iter; left time: 12.3637s\n",
      "Epoch: 5 cost time: 1.2131388187408447\n",
      "Station Epoch: 5, Steps: 225 | Train Loss: 0.8400683 Vali Loss: 0.7150485 Test Loss: 0.3499247\n",
      "Validation loss decreased (0.715731 --> 0.715048).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 1.0100662\n",
      "\tspeed: 0.0240s/iter; left time: 51.6257s\n",
      "\titers: 200, epoch: 6 | loss: 0.6283067\n",
      "\tspeed: 0.0176s/iter; left time: 36.1417s\n",
      "Epoch: 6 cost time: 4.017696380615234\n",
      "Backbone Epoch: 1, Steps: 225 | Train Loss: 0.9767400 Vali Loss: 0.7395227 Test Loss: 0.4435011\n",
      "Validation loss decreased (inf --> 0.739523).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 7 | loss: 1.0917602\n",
      "\tspeed: 0.0322s/iter; left time: 62.0980s\n",
      "\titers: 200, epoch: 7 | loss: 0.6679821\n",
      "\tspeed: 0.0180s/iter; left time: 32.8762s\n",
      "Epoch: 7 cost time: 4.068219184875488\n",
      "Backbone Epoch: 2, Steps: 225 | Train Loss: 0.9089204 Vali Loss: 0.7322854 Test Loss: 0.4288523\n",
      "Validation loss decreased (0.739523 --> 0.732285).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.7607693\n",
      "\tspeed: 0.0330s/iter; left time: 56.2099s\n",
      "\titers: 200, epoch: 8 | loss: 1.1101314\n",
      "\tspeed: 0.0187s/iter; left time: 29.9483s\n",
      "Epoch: 8 cost time: 4.216555118560791\n",
      "Backbone Epoch: 3, Steps: 225 | Train Loss: 0.8916649 Vali Loss: 0.7187335 Test Loss: 0.3999175\n",
      "Validation loss decreased (0.732285 --> 0.718734).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 9 | loss: 1.0895000\n",
      "\tspeed: 0.0313s/iter; left time: 46.2238s\n",
      "\titers: 200, epoch: 9 | loss: 0.6937146\n",
      "\tspeed: 0.0177s/iter; left time: 24.2971s\n",
      "Epoch: 9 cost time: 3.9259064197540283\n",
      "Backbone Epoch: 4, Steps: 225 | Train Loss: 0.8624049 Vali Loss: 0.7083221 Test Loss: 0.3810974\n",
      "Validation loss decreased (0.718734 --> 0.708322).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.8454124\n",
      "\tspeed: 0.0321s/iter; left time: 40.2056s\n",
      "\titers: 200, epoch: 10 | loss: 0.8007569\n",
      "\tspeed: 0.0175s/iter; left time: 20.1722s\n",
      "Epoch: 10 cost time: 4.020641326904297\n",
      "Backbone Epoch: 5, Steps: 225 | Train Loss: 0.8317907 Vali Loss: 0.7071365 Test Loss: 0.3729743\n",
      "Validation loss decreased (0.708322 --> 0.707137).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 11 | loss: 1.2636704\n",
      "\tspeed: 0.0307s/iter; left time: 31.5242s\n",
      "\titers: 200, epoch: 11 | loss: 0.5371707\n",
      "\tspeed: 0.0172s/iter; left time: 15.8832s\n",
      "Epoch: 11 cost time: 3.847550392150879\n",
      "Backbone Epoch: 6, Steps: 225 | Train Loss: 0.8103570 Vali Loss: 0.7118289 Test Loss: 0.3720203\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.8765784\n",
      "\tspeed: 0.0315s/iter; left time: 25.2184s\n",
      "\titers: 200, epoch: 12 | loss: 0.5144888\n",
      "\tspeed: 0.0172s/iter; left time: 12.0404s\n",
      "Epoch: 12 cost time: 3.8610472679138184\n",
      "Backbone Epoch: 7, Steps: 225 | Train Loss: 0.7994788 Vali Loss: 0.7165598 Test Loss: 0.3724926\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.8843291\n",
      "\tspeed: 0.0307s/iter; left time: 17.6630s\n",
      "\titers: 200, epoch: 13 | loss: 0.7674705\n",
      "\tspeed: 0.0171s/iter; left time: 8.1437s\n",
      "Epoch: 13 cost time: 3.8360366821289062\n",
      "Backbone Epoch: 8, Steps: 225 | Train Loss: 0.7953129 Vali Loss: 0.7174545 Test Loss: 0.3723657\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "Average time per pretrain epoch: 1.66244s\n",
      "Average time per joint train epoch: 4.94965s\n",
      "test 2161\n",
      "mse:0.3713555634021759, mae:0.43030044436454773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37135556, 0.43030044)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.root_path='datasets'\n",
    "configs.data='custom'\n",
    "configs.data_path='electricity.csv'\n",
    "\n",
    "configs.model_name=\"iTransformer\"\n",
    "if configs.model_name==\"iTransformer\":\n",
    "    configs.d_model=128\n",
    "    configs.d_ff=128\n",
    "    configs.factor=1\n",
    "elif configs.model_name in [\"Autoformer\", \"FEDformer\"]:\n",
    "    configs.d_model=512\n",
    "    configs.d_ff=2048\n",
    "    configs.factor=3\n",
    "    \n",
    "configs.learning_rate=0.0001\n",
    "configs.seq_len=720\n",
    "configs.label_len=168\n",
    "configs.pred_len=720\n",
    "configs.enc_in=7\n",
    "configs.dec_in=7\n",
    "configs.c_out=7\n",
    "\n",
    "configs.use_norm='lt'\n",
    "configs.station_pre_lr=0.00001\n",
    "configs.station_joint_lr=0.00001\n",
    "configs.t_ff=64\n",
    "configs.alpha=0.2\n",
    "configs.twice_epoch=3\n",
    "configs.use_mlp=True\n",
    "configs.t_norm=True\n",
    "configs.s_norm=False\n",
    "configs.affine=True\n",
    "\n",
    "decomp = ema_decomp(configs.alpha)\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_pre_lr)\n",
    "\n",
    "setting = 'test'\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 결과 포맷팅\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('result.csv')\n",
    "result_list = []\n",
    "for i in range(len(df)):\n",
    "    settings = df.Setting[i].split('_')\n",
    "    dataset = settings[1]\n",
    "    model_name = settings[4]\n",
    "    seq_len = settings[7][2:]\n",
    "    pred_len = settings[9][2:]\n",
    "    trend_norm = settings[-4][2:]\n",
    "    station_pre_lr = settings[-3][3:]\n",
    "    use_mlp = settings[-2][2:]\n",
    "    mse = df.MSE[i]\n",
    "    mae = df.MAE[i]\n",
    "    \n",
    "    result_list.append([dataset, model_name, seq_len, pred_len, trend_norm, station_pre_lr, use_mlp, mse, mae])\n",
    "df_result = pd.DataFrame(result_list, columns=['Dataset', 'Model', 'Seq_Len', 'Pred_Len', 'Trend_Norm', 'Station_LR', 'Use_MLP', 'MSE', 'MAE'])\n",
    "df_result.to_csv('summary_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7585\n"
     ]
    }
   ],
   "source": [
    "data_set, data_loader = data_provider(configs, flag='train')\n",
    "\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = next(iter(data_loader))\n",
    "\n",
    "decomp = envelope_decomp(25).to(device)\n",
    "seasonal_x, trend_x = decomp(batch_y)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(batch_y[0, :, -1].cpu().numpy(), label='Original Series')\n",
    "plt.plot(trend_x[0, :, -1].cpu().numpy(), label='Extracted Trend', linestyle='--')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(seasonal_x[0, :, -1].cpu().numpy(), label='Extracted Seasonal')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.savefig('decomposition_example.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_x, batch_y, batch_x_mark, batch_y_mark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdata_loader\u001b[49m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# visualize column 0~9\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = data_provider(configs, flag='train')[1]\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = next(iter(train_loader))\n",
    "\n",
    "# visualize column 0~9\n",
    "for col in range(10):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(batch_y[0, :, col].cpu().numpy(), label='Original Series')\n",
    "    seasonal_x, trend_x = decomp(batch_y)\n",
    "    plt.plot(trend_x[0, :, col].cpu().numpy(), label='Extracted Trend', linestyle='--')\n",
    "    plt.plot(seasonal_x[0, :, col].cpu().numpy(), label='Extracted Seasonal', linestyle=':')\n",
    "    plt.title(f'Column {col} Decomposition')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'decomposition_column_{col}.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
