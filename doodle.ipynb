{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from types import SimpleNamespace\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "\n",
    "from utils.metrics import metric\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from data_provider.data_loader import Dataset_Custom, Dataset_ETT_hour, Dataset_ETT_minute\n",
    "from models import DLinear, iTransformer, Autoformer, FEDformer, Linear, LSTM\n",
    "from models.trend import exp_1, exp_2, exp_3, exp_4\n",
    "from normalizers import SAN, DDN, TP, RevIN, NoNorm, TP2, LightTrend, SAN_exp, DDN_exp\n",
    "from layers.decomposition import series_decomp, fft_topk_decomp\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    task_name = \"long_term_forecast\",\n",
    "    model_name = \"iTransformer\", \n",
    "    \n",
    "    # Task\n",
    "    seq_len=720,\n",
    "    label_len=168,\n",
    "    pred_len=96,\n",
    "    enc_in=321,\n",
    "    dec_in=321,\n",
    "    c_out=321,\n",
    "    features='M',\n",
    "    freq='h',\n",
    "    \n",
    "    # Data\n",
    "    root_path='datasets',\n",
    "    data='custom',\n",
    "    data_path='electricity.csv',\n",
    "    target='OT',\n",
    "    batch_size=16,\n",
    "    \n",
    "    # Basic Model Settings\n",
    "    checkpoints='./manual_exp/checkpoints/',\n",
    "    dropout=0.1,\n",
    "    embed='timeF',\n",
    "    output_attention=False,\n",
    "    activation='gelu',\n",
    "    moving_avg=25,\n",
    "    num_kernels=6,\n",
    "    individual=False,\n",
    "    learning_rate=0.0005,\n",
    "    n_heads=8,\n",
    "    patience=3,\n",
    "    train_epochs=10,\n",
    "    use_amp=False,\n",
    "    lradj='type1',\n",
    "    # d_model=512,\n",
    "    # d_ff=2048,\n",
    "    \n",
    "    # iTransformer\n",
    "    d_model=128,\n",
    "    d_ff=128,\n",
    "    factor=1,\n",
    "    e_layers=2,\n",
    "    class_strategy='projection',\n",
    "    \n",
    "    # Autoformer\n",
    "    # factor=3,\n",
    "    # e_layers=2,\n",
    "    # d_layers=1,\n",
    "    \n",
    "    # FEDformer\n",
    "    # factor=3,\n",
    "    # e_layers=2,\n",
    "    # d_layers=1,\n",
    "    \n",
    "    # Normalizer\n",
    "    use_norm='lt',\n",
    "    norm_variant='original',\n",
    "    station_type='adaptive',\n",
    "    station_lr=0.001,\n",
    "    alpha=0.3,\n",
    "    s_norm=True,\n",
    "    t_norm=True,\n",
    "    use_mlp=True,\n",
    "    affine=True,\n",
    "    period_len=24,\n",
    "    pre_epoch=5,\n",
    "    twice_epoch=2,\n",
    "    j=1,\n",
    "    learnable=False,\n",
    "    wavelet='coif3',\n",
    "    dr=0.05,\n",
    "    kernel_len=7,\n",
    "    hkernel_len=5,\n",
    "    pd_ff=128,\n",
    "    pd_model=128,\n",
    "    pe_layers=0,\n",
    "    kernel_size=25,\n",
    "    reinit_mean_pretrain=False,\n",
    "    reinit_std_pretrain=False,\n",
    ")\n",
    "model_dict={\n",
    "    'DLinear': DLinear,\n",
    "    'iTransformer': iTransformer,\n",
    "    'Autoformer': Autoformer,\n",
    "    'FEDformer': FEDformer,\n",
    "    'Linear': Linear,\n",
    "    'LSTM': LSTM,\n",
    "    'exp_1': exp_1,\n",
    "    'exp_2': exp_2,\n",
    "    'exp_3': exp_3,\n",
    "    'exp_4': exp_4,\n",
    "}\n",
    "linear_models = ['DLinear', 'Linear', 'LSTM', 'exp_1', 'exp_2', 'exp_3', 'exp_4']\n",
    "norm_dict = {\n",
    "    'none': NoNorm,\n",
    "    'revin': RevIN,\n",
    "    'san': SAN,\n",
    "    'san_exp': SAN_exp,\n",
    "    'ddn': DDN,\n",
    "    'ddn_exp': DDN_exp,\n",
    "    'tp': TP,\n",
    "    'tp2': TP2,\n",
    "    'lt': LightTrend,\n",
    "}\n",
    "decomp = series_decomp(25)\n",
    "# decomp = fft_topk_decomp(5)\n",
    "\n",
    "# SAN\n",
    "def san_loss(y, statistics_pred):\n",
    "    bs, len, dim = y.shape\n",
    "    y = y.reshape(bs, -1, configs.period_len, dim)\n",
    "    mean = torch.mean(y, dim=2)\n",
    "    std = torch.std(y, dim=2)\n",
    "    station_ture = torch.cat([mean, std], dim=-1)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# DDN\n",
    "def ddn_loss(y, statistics_pred):\n",
    "    _, (mean, std) = norm.norm(y.transpose(-1, -2), False)\n",
    "    station_ture = torch.cat([mean, std], dim=1).transpose(-1, -2)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# TREAD\n",
    "def tread_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred[-1]\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "# LightTrend\n",
    "def lt_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "station_loss_dict = {\n",
    "    'none': None,\n",
    "    'revin': None,\n",
    "    'san': san_loss,\n",
    "    'ddn': ddn_loss,\n",
    "    'san_exp': san_loss,\n",
    "    'ddn_exp': ddn_loss,\n",
    "    'tp': tread_loss,\n",
    "    'tp2': tread_loss,\n",
    "    'lt': lt_loss,\n",
    "}\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "\n",
    "# [pre train, pre epoch, joint train, join epoch]\n",
    "station_setting_dict = {\n",
    "    'none': [0, 0, 0, 0],\n",
    "    'revin': [0, 0, 0, 0],\n",
    "    'san': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp2': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'lt': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'san_exp': [1, configs.pre_epoch, 0, 0],\n",
    "    'ddn_exp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "}\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 총 학습 가능 파라미터 수: 14,433 개\n",
      "모델 파라미터의 예상 메모리 크기: 0.055 MB\n"
     ]
    }
   ],
   "source": [
    "# 학습 가능한 총 파라미터 수 계산\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in norm.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"모델의 총 학습 가능 파라미터 수: {trainable_params:,} 개\")\n",
    "\n",
    "# 파라미터 저장을 위한 메모리 크기 계산 (기본 float32 기준)\n",
    "# 1 float32 = 4 bytes\n",
    "param_size_bytes = trainable_params * 4\n",
    "param_size_mb = param_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"모델 파라미터의 예상 메모리 크기: {param_size_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion, epoch):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # Seasonal or Trend\n",
    "            seasonal_x, trend_x = decomp(batch_x)\n",
    "            seasonal_y, trend_y = decomp(batch_y)\n",
    "            # batch_x = trend_x\n",
    "            # batch_y = trend_y\n",
    "            # batch_x = seasonal_x\n",
    "            # batch_y = seasonal_y\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "\n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float()\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "            total_loss.append(loss.cpu().item())\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    norm.train()\n",
    "    return total_loss\n",
    "\n",
    "def _get_data(flag):\n",
    "    data_set, data_loader = data_provider(configs, flag)\n",
    "    return data_set, data_loader\n",
    "\n",
    "def train(setting):\n",
    "    train_data, train_loader = _get_data(flag='train')\n",
    "    vali_data, vali_loader = _get_data(flag='val')\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    path = os.path.join(configs.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    path_station = './station/' + '{}_s{}_p{}'.format(configs.use_norm, configs.data,\n",
    "                                                        configs.seq_len, configs.pred_len)\n",
    "    if not os.path.exists(path_station):\n",
    "        os.makedirs(path_station)\n",
    "\n",
    "    time_now = time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "    early_stopping_station_model = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "\n",
    "    if configs.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    ### NEW ###\n",
    "    # 재초기화를 위한 헬퍼 함수 정의\n",
    "    def reinit_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters() # PyTorch 기본값으로 가중치 초기화\n",
    "    ### END NEW ###\n",
    "\n",
    "    time_per_epoch = []\n",
    "    for epoch in range(configs.train_epochs + station_setting[1]):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        epoch_start = time()\n",
    "        # Load best station model after pretraining\n",
    "        if station_setting[0] > 0 and epoch == station_setting[1]:\n",
    "            best_model_path = path_station + '/' + 'checkpoint.pth'\n",
    "            norm.load_state_dict(torch.load(best_model_path))\n",
    "            print('loading pretrained adaptive station model')\n",
    "            \n",
    "            ### NEW ###\n",
    "            # --- Motivating Experiment 3.2: 학습 분리 효과 검증 ---\n",
    "            # 'DDN + w/o trend pretrain' 실험\n",
    "            if getattr(configs, 'reinit_mean_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing MEAN predictor weights !!!\")\n",
    "                norm.mlp.m_project.apply(reinit_weights)\n",
    "                norm.mlp.mean_proj.apply(reinit_weights)\n",
    "                norm.mlp.m_concat.apply(reinit_weights)\n",
    "                norm.mlp.mean_ffn.apply(reinit_weights)\n",
    "                norm.mlp.mean_pred.apply(reinit_weights)\n",
    "                print(\"!!! Mean predictor re-initialization complete. !!!\")\n",
    "\n",
    "            # 'DDN + w/o seasonal pretrain' (std) 실험\n",
    "            if getattr(configs, 'reinit_std_pretrain', False):\n",
    "                print(\"!!! EXPERIMENT: Re-initializing STD predictor weights !!!\")\n",
    "                norm.mlp.s_project.apply(reinit_weights)\n",
    "                norm.mlp.std_proj.apply(reinit_weights)\n",
    "                norm.mlp.s_concat.apply(reinit_weights)\n",
    "                norm.mlp.std_ffn.apply(reinit_weights)\n",
    "                norm.mlp.std_pred.apply(reinit_weights)\n",
    "                print(\"!!! Std predictor re-initialization complete. !!!\")\n",
    "            \n",
    "            # 'DDN + w/o pretrain' 실험은 station_setting[1] = 0 으로 설정하고\n",
    "            # 이 if 블록에 들어오지 않게 함으로써 자연스럽게 구현됩니다.\n",
    "            ### END NEW ###\n",
    "        \n",
    "        # Add station parameters to model optim after pretraining and delay epochs for joint training\n",
    "        if station_setting[2] > 0 and station_setting[3] == epoch - station_setting[1]:\n",
    "            lr = model_optim.param_groups[0]['lr']\n",
    "            model_optim.add_param_group({'params': norm.parameters(), 'lr': lr})\n",
    "        \n",
    "        model.train()\n",
    "        norm.train()\n",
    "        epoch_time = time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # Seasonal or Trend\n",
    "            seasonal_x, trend_x = decomp(batch_x)\n",
    "            seasonal_y, trend_y = decomp(batch_y)\n",
    "            # batch_x = trend_x\n",
    "            # batch_y = trend_y\n",
    "            # batch_x = seasonal_x\n",
    "            # batch_y = seasonal_y\n",
    "            \n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            \n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                        f_dim = -1 if configs.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                    f_dim = -1 if configs.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                    if configs.features == 'MS':\n",
    "                        statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                        \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time() - time_now) / iter_count\n",
    "                left_time = speed * (\n",
    "                        (configs.train_epochs + station_setting[1] - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time()\n",
    "            if configs.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                # two-stage training schema\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    norm_optim.step()\n",
    "                else:\n",
    "                    model_optim.step()\n",
    "                model_optim.zero_grad()\n",
    "                norm_optim.zero_grad()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion, epoch)\n",
    "        test_loss = vali(test_data, test_loader, criterion, epoch)\n",
    "\n",
    "        if epoch + 1 <= station_setting[1]:\n",
    "            print(\n",
    "                \"Station Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping_station_model(vali_loss, norm, path_station)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1, configs, configs.station_lr)\n",
    "        else:\n",
    "            print(\n",
    "                \"Backbone Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1 - station_setting[1], train_steps, train_loss, vali_loss, test_loss))\n",
    "            # if: joint training, else: only model training\n",
    "            if station_setting[2] > 0 and station_setting[3] <= epoch - station_setting[1]:\n",
    "                early_stopping(vali_loss, model, path, norm, path_station)\n",
    "            else:\n",
    "                early_stopping(vali_loss, model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(model_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.learning_rate)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.station_lr)\n",
    "        time_per_epoch.append(time() - epoch_start)\n",
    "    print(\"Average time per pretrain epoch: {:.5f}s\".format(np.mean(time_per_epoch[:5])))\n",
    "    print(\"Average time per joint train epoch: {:.5f}s\".format(np.mean(time_per_epoch[5:])))\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if station_setting[2] > 0:\n",
    "        norm.load_state_dict(torch.load(path_station + '/' + 'checkpoint.pth'))\n",
    "    return model\n",
    "\n",
    "def check_stationarity(batch_x, test='adf'):\n",
    "    ts = batch_x[0, :, -1]\n",
    "    if test == 'adf':\n",
    "        stat, pvalue, *_ = adfuller(ts)\n",
    "        # 낮은 p-value일수록 정상성 ↑ → 점수는 1 - pvalue\n",
    "        score = 1 - min(pvalue, 1.0)\n",
    "    elif test == 'kpss':\n",
    "        stat, pvalue, *_ = kpss(ts, nlags=\"auto\")\n",
    "        # 높은 p-value일수록 정상성 ↑\n",
    "        score = min(pvalue, 1.0)\n",
    "    return score\n",
    "\n",
    "def test(setting, test=0):\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    if test:\n",
    "        print('loading model')\n",
    "        model.load_state_dict(torch.load(os.path.join('./manual_exp/checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    inputx = []\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            input_x = batch_x\n",
    "            \n",
    "            # Seasonal or Trend\n",
    "            seasonal_x, trend_x = decomp(batch_x)\n",
    "            seasonal_y, trend_y = decomp(batch_y)\n",
    "            # batch_x = trend_x\n",
    "            # batch_y = trend_y\n",
    "            # batch_x = seasonal_x\n",
    "            # batch_y = seasonal_y\n",
    "\n",
    "            # normalize\n",
    "            input_noNorm = batch_x\n",
    "            if configs.use_norm == 'ddn' or configs.use_norm == 'ddn_exp':\n",
    "                batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            input_withNorm = batch_x\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder x\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "            dec_label = batch_x[:, -configs.label_len:, :]\n",
    "            dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "            # encoder - decoder\n",
    "            if configs.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            else:\n",
    "                if configs.model_name in linear_models:\n",
    "                    outputs = model(batch_x)\n",
    "                else:\n",
    "                    if configs.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "            f_dim = -1 if configs.features == 'MS' else 0\n",
    "            outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "            if configs.features == 'MS':\n",
    "                statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                \n",
    "            # de-normalize\n",
    "            output_noDenorm = outputs\n",
    "            outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "            output_withDenorm = outputs\n",
    "            \n",
    "            batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "            pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "            true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "            inputx.append(batch_x.detach().cpu().numpy())\n",
    "            if i % 20 == 0:\n",
    "                x = input_x.detach().cpu().numpy()\n",
    "                gt = np.concatenate((x[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                pd = np.concatenate((x[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "            \n",
    "            # visualize input and output with/without normalization/denormalization\n",
    "            if i == 0:\n",
    "                i_n = input_noNorm.detach().cpu().numpy()\n",
    "                i_w = input_withNorm.detach().cpu().numpy()\n",
    "                o_n = output_noDenorm.detach().cpu().numpy()\n",
    "                o_w = output_withDenorm.detach().cpu().numpy()\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                # plt.subplot(3, 1, 1)\n",
    "                # plt.plot(i_w[0, :, -1], label='Normalized', color='darkorange')\n",
    "                # plt.plot(i_n[0, :, -1], label='Raw')\n",
    "                # plt.legend()\n",
    "                # plt.grid()\n",
    "                \n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(i_n[0, :, -1], label='Original')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(i_w[0, :, -1], label='Normalized')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_input.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='Ground Truth', color='black', linewidth=2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(0, label='Before Norm(Model Output)')\n",
    "                # legend outside of plot\n",
    "                plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3)\n",
    "                \n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(o_n[0, :, -1], label='Before Norm(Model Output)')\n",
    "                # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_output.pdf'))\n",
    "\n",
    "    # if configs.test_flop:\n",
    "    #     test_params_flop((batch_x.shape[1], batch_x.shape[2]))\n",
    "    #     exit()\n",
    "    preds = np.array(preds, dtype=object)\n",
    "    trues = np.array(trues, dtype=object)\n",
    "    # inputx = np.array(inputx)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    # inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "    # result save\n",
    "    folder_path = './manual_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "    print('mse:{}, mae:{}'.format(mse, mae))\n",
    "    f = open(\"result.txt\", 'a')\n",
    "    f.write(setting + \"  \\n\")\n",
    "    f.write('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n",
    "    # np.save(folder_path + 'pred.npy', preds)\n",
    "    # np.save(folder_path + 'true.npy', trues)\n",
    "    # np.save(folder_path + 'x.npy', inputx)\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 17597\n",
      "val 2537\n",
      "test 5165\n",
      "\titers: 100, epoch: 1 | loss: 0.0584336\n",
      "\tspeed: 0.0429s/iter; left time: 703.0683s\n",
      "\titers: 200, epoch: 1 | loss: 0.0720515\n",
      "\tspeed: 0.0221s/iter; left time: 359.9234s\n",
      "\titers: 300, epoch: 1 | loss: 0.0823734\n",
      "\tspeed: 0.0219s/iter; left time: 354.1596s\n",
      "\titers: 400, epoch: 1 | loss: 0.0529639\n",
      "\tspeed: 0.0215s/iter; left time: 346.3670s\n",
      "\titers: 500, epoch: 1 | loss: 0.0551698\n",
      "\tspeed: 0.0217s/iter; left time: 347.1853s\n",
      "\titers: 600, epoch: 1 | loss: 0.0649555\n",
      "\tspeed: 0.0212s/iter; left time: 337.1318s\n",
      "\titers: 700, epoch: 1 | loss: 0.0664762\n",
      "\tspeed: 0.0224s/iter; left time: 352.9576s\n",
      "\titers: 800, epoch: 1 | loss: 0.0574967\n",
      "\tspeed: 0.0224s/iter; left time: 350.6515s\n",
      "\titers: 900, epoch: 1 | loss: 0.0643788\n",
      "\tspeed: 0.0219s/iter; left time: 341.6106s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0658016\n",
      "\tspeed: 0.0223s/iter; left time: 345.2110s\n",
      "Epoch: 1 cost time: 26.215226888656616\n",
      "Station Epoch: 1, Steps: 1099 | Train Loss: 0.0645538 Vali Loss: 0.0441553 Test Loss: 0.0640515\n",
      "Validation loss decreased (inf --> 0.044155).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.0505703\n",
      "\tspeed: 0.1438s/iter; left time: 2198.9960s\n",
      "\titers: 200, epoch: 2 | loss: 0.0681897\n",
      "\tspeed: 0.0224s/iter; left time: 340.6928s\n",
      "\titers: 300, epoch: 2 | loss: 0.0826595\n",
      "\tspeed: 0.0224s/iter; left time: 337.7282s\n",
      "\titers: 400, epoch: 2 | loss: 0.0475417\n",
      "\tspeed: 0.0221s/iter; left time: 330.8750s\n",
      "\titers: 500, epoch: 2 | loss: 0.0611194\n",
      "\tspeed: 0.0229s/iter; left time: 341.0344s\n",
      "\titers: 600, epoch: 2 | loss: 0.0584665\n",
      "\tspeed: 0.0222s/iter; left time: 327.8306s\n",
      "\titers: 700, epoch: 2 | loss: 0.0615413\n",
      "\tspeed: 0.0219s/iter; left time: 321.6027s\n",
      "\titers: 800, epoch: 2 | loss: 0.0490385\n",
      "\tspeed: 0.0219s/iter; left time: 319.0649s\n",
      "\titers: 900, epoch: 2 | loss: 0.0525253\n",
      "\tspeed: 0.0215s/iter; left time: 312.0101s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0576900\n",
      "\tspeed: 0.0223s/iter; left time: 320.2482s\n",
      "Epoch: 2 cost time: 24.422443628311157\n",
      "Station Epoch: 2, Steps: 1099 | Train Loss: 0.0589675 Vali Loss: 0.0429195 Test Loss: 0.0620391\n",
      "Validation loss decreased (0.044155 --> 0.042919).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.0650954\n",
      "\tspeed: 0.1387s/iter; left time: 1967.5911s\n",
      "\titers: 200, epoch: 3 | loss: 0.0563855\n",
      "\tspeed: 0.0209s/iter; left time: 294.7714s\n",
      "\titers: 300, epoch: 3 | loss: 0.0511982\n",
      "\tspeed: 0.0216s/iter; left time: 302.1053s\n",
      "\titers: 400, epoch: 3 | loss: 0.0564147\n",
      "\tspeed: 0.0224s/iter; left time: 310.8540s\n",
      "\titers: 500, epoch: 3 | loss: 0.0583382\n",
      "\tspeed: 0.0217s/iter; left time: 299.1139s\n",
      "\titers: 600, epoch: 3 | loss: 0.0398007\n",
      "\tspeed: 0.0216s/iter; left time: 295.7362s\n",
      "\titers: 700, epoch: 3 | loss: 0.0762429\n",
      "\tspeed: 0.0211s/iter; left time: 287.3711s\n",
      "\titers: 800, epoch: 3 | loss: 0.0533078\n",
      "\tspeed: 0.0215s/iter; left time: 290.0736s\n",
      "\titers: 900, epoch: 3 | loss: 0.0466556\n",
      "\tspeed: 0.0216s/iter; left time: 289.3056s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0522692\n",
      "\tspeed: 0.0210s/iter; left time: 278.6648s\n",
      "Epoch: 3 cost time: 23.575276136398315\n",
      "Station Epoch: 3, Steps: 1099 | Train Loss: 0.0569437 Vali Loss: 0.0420336 Test Loss: 0.0616015\n",
      "Validation loss decreased (0.042919 --> 0.042034).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.0951035\n",
      "\tspeed: 0.1323s/iter; left time: 1732.1046s\n",
      "\titers: 200, epoch: 4 | loss: 0.1144146\n",
      "\tspeed: 0.0234s/iter; left time: 304.3791s\n",
      "\titers: 300, epoch: 4 | loss: 0.0507950\n",
      "\tspeed: 0.0224s/iter; left time: 289.3464s\n",
      "\titers: 400, epoch: 4 | loss: 0.0561360\n",
      "\tspeed: 0.0219s/iter; left time: 279.8567s\n",
      "\titers: 500, epoch: 4 | loss: 0.0472437\n",
      "\tspeed: 0.0223s/iter; left time: 283.3761s\n",
      "\titers: 600, epoch: 4 | loss: 0.0697457\n",
      "\tspeed: 0.0230s/iter; left time: 288.9595s\n",
      "\titers: 700, epoch: 4 | loss: 0.0464420\n",
      "\tspeed: 0.0229s/iter; left time: 286.5188s\n",
      "\titers: 800, epoch: 4 | loss: 0.0481616\n",
      "\tspeed: 0.0229s/iter; left time: 284.1790s\n",
      "\titers: 900, epoch: 4 | loss: 0.0473158\n",
      "\tspeed: 0.0223s/iter; left time: 273.9241s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0432732\n",
      "\tspeed: 0.0223s/iter; left time: 271.5967s\n",
      "Epoch: 4 cost time: 24.801641941070557\n",
      "Station Epoch: 4, Steps: 1099 | Train Loss: 0.0561080 Vali Loss: 0.0418810 Test Loss: 0.0613655\n",
      "Validation loss decreased (0.042034 --> 0.041881).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.0458189\n",
      "\tspeed: 0.1461s/iter; left time: 1751.3478s\n",
      "\titers: 200, epoch: 5 | loss: 0.0845377\n",
      "\tspeed: 0.0221s/iter; left time: 263.1399s\n",
      "\titers: 300, epoch: 5 | loss: 0.0471832\n",
      "\tspeed: 0.0227s/iter; left time: 267.9024s\n",
      "\titers: 400, epoch: 5 | loss: 0.0470758\n",
      "\tspeed: 0.0224s/iter; left time: 261.4070s\n",
      "\titers: 500, epoch: 5 | loss: 0.0745365\n",
      "\tspeed: 0.0226s/iter; left time: 261.7592s\n",
      "\titers: 600, epoch: 5 | loss: 0.0596691\n",
      "\tspeed: 0.0216s/iter; left time: 248.1540s\n",
      "\titers: 700, epoch: 5 | loss: 0.0605916\n",
      "\tspeed: 0.0230s/iter; left time: 261.8588s\n",
      "\titers: 800, epoch: 5 | loss: 0.0470711\n",
      "\tspeed: 0.0220s/iter; left time: 248.7146s\n",
      "\titers: 900, epoch: 5 | loss: 0.0407304\n",
      "\tspeed: 0.0223s/iter; left time: 249.3343s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0593028\n",
      "\tspeed: 0.0225s/iter; left time: 249.6912s\n",
      "Epoch: 5 cost time: 24.529292345046997\n",
      "Station Epoch: 5, Steps: 1099 | Train Loss: 0.0557233 Vali Loss: 0.0416791 Test Loss: 0.0614016\n",
      "Validation loss decreased (0.041881 --> 0.041679).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "loading pretrained adaptive station model\n",
      "\titers: 100, epoch: 6 | loss: 0.2084622\n",
      "\tspeed: 0.1886s/iter; left time: 2054.3976s\n",
      "\titers: 200, epoch: 6 | loss: 0.1607976\n",
      "\tspeed: 0.0434s/iter; left time: 467.8767s\n",
      "\titers: 300, epoch: 6 | loss: 0.1383882\n",
      "\tspeed: 0.0440s/iter; left time: 470.2133s\n",
      "\titers: 400, epoch: 6 | loss: 0.1766500\n",
      "\tspeed: 0.0438s/iter; left time: 463.4512s\n",
      "\titers: 500, epoch: 6 | loss: 0.1419852\n",
      "\tspeed: 0.0436s/iter; left time: 457.5099s\n",
      "\titers: 600, epoch: 6 | loss: 0.1391267\n",
      "\tspeed: 0.0431s/iter; left time: 447.8048s\n",
      "\titers: 700, epoch: 6 | loss: 0.1360238\n",
      "\tspeed: 0.0435s/iter; left time: 448.0461s\n",
      "\titers: 800, epoch: 6 | loss: 0.1591999\n",
      "\tspeed: 0.0432s/iter; left time: 440.0151s\n",
      "\titers: 900, epoch: 6 | loss: 0.1273851\n",
      "\tspeed: 0.0433s/iter; left time: 436.9136s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1184374\n",
      "\tspeed: 0.0434s/iter; left time: 433.7320s\n",
      "Epoch: 6 cost time: 50.458054542541504\n",
      "Backbone Epoch: 1, Steps: 1099 | Train Loss: 0.1599938 Vali Loss: 0.1177670 Test Loss: 0.1386359\n",
      "Validation loss decreased (inf --> 0.117767).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 7 | loss: 0.1125585\n",
      "\tspeed: 0.2392s/iter; left time: 2342.3689s\n",
      "\titers: 200, epoch: 7 | loss: 0.1406505\n",
      "\tspeed: 0.0442s/iter; left time: 428.3187s\n",
      "\titers: 300, epoch: 7 | loss: 0.1804538\n",
      "\tspeed: 0.0432s/iter; left time: 414.7808s\n",
      "\titers: 400, epoch: 7 | loss: 0.1408674\n",
      "\tspeed: 0.0436s/iter; left time: 413.5703s\n",
      "\titers: 500, epoch: 7 | loss: 0.1772052\n",
      "\tspeed: 0.0432s/iter; left time: 406.0671s\n",
      "\titers: 600, epoch: 7 | loss: 0.1332670\n",
      "\tspeed: 0.0434s/iter; left time: 402.8742s\n",
      "\titers: 700, epoch: 7 | loss: 0.1480704\n",
      "\tspeed: 0.0433s/iter; left time: 397.7024s\n",
      "\titers: 800, epoch: 7 | loss: 0.1259896\n",
      "\tspeed: 0.0431s/iter; left time: 392.2175s\n",
      "\titers: 900, epoch: 7 | loss: 0.1086153\n",
      "\tspeed: 0.0432s/iter; left time: 388.7229s\n",
      "\titers: 1000, epoch: 7 | loss: 0.1344698\n",
      "\tspeed: 0.0431s/iter; left time: 382.9422s\n",
      "Epoch: 7 cost time: 47.67330241203308\n",
      "Backbone Epoch: 2, Steps: 1099 | Train Loss: 0.1388618 Vali Loss: 0.1147766 Test Loss: 0.1353219\n",
      "Validation loss decreased (0.117767 --> 0.114777).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 8 | loss: 0.1365157\n",
      "\tspeed: 0.2403s/iter; left time: 2089.1668s\n",
      "\titers: 200, epoch: 8 | loss: 0.1682175\n",
      "\tspeed: 0.0434s/iter; left time: 373.2730s\n",
      "\titers: 300, epoch: 8 | loss: 0.1223532\n",
      "\tspeed: 0.0432s/iter; left time: 366.8756s\n",
      "\titers: 400, epoch: 8 | loss: 0.1327491\n",
      "\tspeed: 0.0432s/iter; left time: 362.6704s\n",
      "\titers: 500, epoch: 8 | loss: 0.1165857\n",
      "\tspeed: 0.0434s/iter; left time: 360.2894s\n",
      "\titers: 600, epoch: 8 | loss: 0.1064898\n",
      "\tspeed: 0.0436s/iter; left time: 357.1715s\n",
      "\titers: 700, epoch: 8 | loss: 0.1278855\n",
      "\tspeed: 0.0434s/iter; left time: 351.6200s\n",
      "\titers: 800, epoch: 8 | loss: 0.1304829\n",
      "\tspeed: 0.0440s/iter; left time: 351.3336s\n",
      "\titers: 900, epoch: 8 | loss: 0.1352324\n",
      "\tspeed: 0.0438s/iter; left time: 345.5108s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1589488\n",
      "\tspeed: 0.0437s/iter; left time: 340.5428s\n",
      "Epoch: 8 cost time: 47.92432522773743\n",
      "Backbone Epoch: 3, Steps: 1099 | Train Loss: 0.1347133 Vali Loss: 0.1121997 Test Loss: 0.1325854\n",
      "Validation loss decreased (0.114777 --> 0.112200).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 0.000125\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 9 | loss: 0.1116692\n",
      "\tspeed: 0.2388s/iter; left time: 1813.3379s\n",
      "\titers: 200, epoch: 9 | loss: 0.1472433\n",
      "\tspeed: 0.0437s/iter; left time: 327.4192s\n",
      "\titers: 300, epoch: 9 | loss: 0.1011657\n",
      "\tspeed: 0.0437s/iter; left time: 323.0464s\n",
      "\titers: 400, epoch: 9 | loss: 0.1313902\n",
      "\tspeed: 0.0437s/iter; left time: 318.5770s\n",
      "\titers: 500, epoch: 9 | loss: 0.1262857\n",
      "\tspeed: 0.0445s/iter; left time: 320.4656s\n",
      "\titers: 600, epoch: 9 | loss: 0.1398056\n",
      "\tspeed: 0.0432s/iter; left time: 306.1373s\n",
      "\titers: 700, epoch: 9 | loss: 0.1211674\n",
      "\tspeed: 0.0434s/iter; left time: 303.8697s\n",
      "\titers: 800, epoch: 9 | loss: 0.1371604\n",
      "\tspeed: 0.0435s/iter; left time: 300.0811s\n",
      "\titers: 900, epoch: 9 | loss: 0.1605314\n",
      "\tspeed: 0.0435s/iter; left time: 295.2148s\n",
      "\titers: 1000, epoch: 9 | loss: 0.1397385\n",
      "\tspeed: 0.0436s/iter; left time: 291.9929s\n",
      "Epoch: 9 cost time: 47.89347290992737\n",
      "Backbone Epoch: 4, Steps: 1099 | Train Loss: 0.1322692 Vali Loss: 0.1113733 Test Loss: 0.1316469\n",
      "Validation loss decreased (0.112200 --> 0.111373).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 6.25e-05\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 10 | loss: 0.1437977\n",
      "\tspeed: 0.2401s/iter; left time: 1559.3193s\n",
      "\titers: 200, epoch: 10 | loss: 0.1501831\n",
      "\tspeed: 0.0433s/iter; left time: 276.6292s\n",
      "\titers: 300, epoch: 10 | loss: 0.1574047\n",
      "\tspeed: 0.0435s/iter; left time: 273.9563s\n",
      "\titers: 400, epoch: 10 | loss: 0.1214350\n",
      "\tspeed: 0.0432s/iter; left time: 267.7050s\n",
      "\titers: 500, epoch: 10 | loss: 0.1236173\n",
      "\tspeed: 0.0435s/iter; left time: 265.4062s\n",
      "\titers: 600, epoch: 10 | loss: 0.1309729\n",
      "\tspeed: 0.0432s/iter; left time: 259.2208s\n",
      "\titers: 700, epoch: 10 | loss: 0.1293715\n",
      "\tspeed: 0.0434s/iter; left time: 255.7848s\n",
      "\titers: 800, epoch: 10 | loss: 0.1337854\n",
      "\tspeed: 0.0436s/iter; left time: 252.8435s\n",
      "\titers: 900, epoch: 10 | loss: 0.1016389\n",
      "\tspeed: 0.0433s/iter; left time: 246.8259s\n",
      "\titers: 1000, epoch: 10 | loss: 0.1255520\n",
      "\tspeed: 0.0437s/iter; left time: 244.2697s\n",
      "Epoch: 10 cost time: 47.754962682724\n",
      "Backbone Epoch: 5, Steps: 1099 | Train Loss: 0.1310189 Vali Loss: 0.1109762 Test Loss: 0.1312120\n",
      "Validation loss decreased (0.111373 --> 0.110976).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 3.125e-05\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.1250943\n",
      "\tspeed: 0.2414s/iter; left time: 1302.7782s\n",
      "\titers: 200, epoch: 11 | loss: 0.1183354\n",
      "\tspeed: 0.0434s/iter; left time: 229.7731s\n",
      "\titers: 300, epoch: 11 | loss: 0.1241019\n",
      "\tspeed: 0.0433s/iter; left time: 225.1396s\n",
      "\titers: 400, epoch: 11 | loss: 0.1456405\n",
      "\tspeed: 0.0435s/iter; left time: 221.7316s\n",
      "\titers: 500, epoch: 11 | loss: 0.1172902\n",
      "\tspeed: 0.0437s/iter; left time: 218.3241s\n",
      "\titers: 600, epoch: 11 | loss: 0.1491014\n",
      "\tspeed: 0.0435s/iter; left time: 212.7997s\n",
      "\titers: 700, epoch: 11 | loss: 0.1226855\n",
      "\tspeed: 0.0436s/iter; left time: 209.0476s\n",
      "\titers: 800, epoch: 11 | loss: 0.1547948\n",
      "\tspeed: 0.0433s/iter; left time: 203.2202s\n",
      "\titers: 900, epoch: 11 | loss: 0.1257481\n",
      "\tspeed: 0.0434s/iter; left time: 199.5700s\n",
      "\titers: 1000, epoch: 11 | loss: 0.1632934\n",
      "\tspeed: 0.0435s/iter; left time: 195.4273s\n",
      "Epoch: 11 cost time: 47.736056089401245\n",
      "Backbone Epoch: 6, Steps: 1099 | Train Loss: 0.1303450 Vali Loss: 0.1105337 Test Loss: 0.1308250\n",
      "Validation loss decreased (0.110976 --> 0.110534).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.5625e-05\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.1461504\n",
      "\tspeed: 0.2396s/iter; left time: 1029.5291s\n",
      "\titers: 200, epoch: 12 | loss: 0.1506418\n",
      "\tspeed: 0.0448s/iter; left time: 188.0628s\n",
      "\titers: 300, epoch: 12 | loss: 0.1379693\n",
      "\tspeed: 0.0440s/iter; left time: 180.4045s\n",
      "\titers: 400, epoch: 12 | loss: 0.1640262\n",
      "\tspeed: 0.0431s/iter; left time: 172.1091s\n",
      "\titers: 500, epoch: 12 | loss: 0.1150574\n",
      "\tspeed: 0.0432s/iter; left time: 168.4242s\n",
      "\titers: 600, epoch: 12 | loss: 0.1604655\n",
      "\tspeed: 0.0432s/iter; left time: 163.9798s\n",
      "\titers: 700, epoch: 12 | loss: 0.1247548\n",
      "\tspeed: 0.0436s/iter; left time: 161.1925s\n",
      "\titers: 800, epoch: 12 | loss: 0.1038495\n",
      "\tspeed: 0.0435s/iter; left time: 156.5116s\n",
      "\titers: 900, epoch: 12 | loss: 0.1262076\n",
      "\tspeed: 0.0432s/iter; left time: 151.1939s\n",
      "\titers: 1000, epoch: 12 | loss: 0.1532809\n",
      "\tspeed: 0.0436s/iter; left time: 147.9675s\n",
      "Epoch: 12 cost time: 47.931928873062134\n",
      "Backbone Epoch: 7, Steps: 1099 | Train Loss: 0.1299613 Vali Loss: 0.1106030 Test Loss: 0.1307950\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.1146458\n",
      "\tspeed: 0.2403s/iter; left time: 768.4628s\n",
      "\titers: 200, epoch: 13 | loss: 0.1222741\n",
      "\tspeed: 0.0436s/iter; left time: 135.2129s\n",
      "\titers: 300, epoch: 13 | loss: 0.1069806\n",
      "\tspeed: 0.0444s/iter; left time: 132.9939s\n",
      "\titers: 400, epoch: 13 | loss: 0.1140877\n",
      "\tspeed: 0.0448s/iter; left time: 129.9276s\n",
      "\titers: 500, epoch: 13 | loss: 0.1057237\n",
      "\tspeed: 0.0439s/iter; left time: 122.9114s\n",
      "\titers: 600, epoch: 13 | loss: 0.1423573\n",
      "\tspeed: 0.0435s/iter; left time: 117.4440s\n",
      "\titers: 700, epoch: 13 | loss: 0.1124426\n",
      "\tspeed: 0.0436s/iter; left time: 113.2397s\n",
      "\titers: 800, epoch: 13 | loss: 0.1291753\n",
      "\tspeed: 0.0437s/iter; left time: 109.1999s\n",
      "\titers: 900, epoch: 13 | loss: 0.1204719\n",
      "\tspeed: 0.0437s/iter; left time: 104.6914s\n",
      "\titers: 1000, epoch: 13 | loss: 0.1273640\n",
      "\tspeed: 0.0438s/iter; left time: 100.7503s\n",
      "Epoch: 13 cost time: 48.16498851776123\n",
      "Backbone Epoch: 8, Steps: 1099 | Train Loss: 0.1297793 Vali Loss: 0.1105756 Test Loss: 0.1306991\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.1229231\n",
      "\tspeed: 0.2402s/iter; left time: 504.0785s\n",
      "\titers: 200, epoch: 14 | loss: 0.1184607\n",
      "\tspeed: 0.0431s/iter; left time: 86.0761s\n",
      "\titers: 300, epoch: 14 | loss: 0.1172999\n",
      "\tspeed: 0.0434s/iter; left time: 82.4326s\n",
      "\titers: 400, epoch: 14 | loss: 0.1512854\n",
      "\tspeed: 0.0436s/iter; left time: 78.3608s\n",
      "\titers: 500, epoch: 14 | loss: 0.1369172\n",
      "\tspeed: 0.0432s/iter; left time: 73.3318s\n",
      "\titers: 600, epoch: 14 | loss: 0.1155380\n",
      "\tspeed: 0.0434s/iter; left time: 69.4674s\n",
      "\titers: 700, epoch: 14 | loss: 0.1664806\n",
      "\tspeed: 0.0440s/iter; left time: 65.9400s\n",
      "\titers: 800, epoch: 14 | loss: 0.1506371\n",
      "\tspeed: 0.0435s/iter; left time: 60.9150s\n",
      "\titers: 900, epoch: 14 | loss: 0.1354812\n",
      "\tspeed: 0.0435s/iter; left time: 56.4992s\n",
      "\titers: 1000, epoch: 14 | loss: 0.1180570\n",
      "\tspeed: 0.0432s/iter; left time: 51.8178s\n",
      "Epoch: 14 cost time: 47.76247310638428\n",
      "Backbone Epoch: 9, Steps: 1099 | Train Loss: 0.1296502 Vali Loss: 0.1104836 Test Loss: 0.1306933\n",
      "Validation loss decreased (0.110534 --> 0.110484).  Saving model ...\n",
      "station model has replaced\n",
      "Updating learning rate to 1.953125e-06\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 15 | loss: 0.1513084\n",
      "\tspeed: 0.2388s/iter; left time: 238.8024s\n",
      "\titers: 200, epoch: 15 | loss: 0.1357388\n",
      "\tspeed: 0.0434s/iter; left time: 39.0865s\n",
      "\titers: 300, epoch: 15 | loss: 0.1335946\n",
      "\tspeed: 0.0433s/iter; left time: 34.6260s\n",
      "\titers: 400, epoch: 15 | loss: 0.1307494\n",
      "\tspeed: 0.0436s/iter; left time: 30.5479s\n",
      "\titers: 500, epoch: 15 | loss: 0.1341915\n",
      "\tspeed: 0.0431s/iter; left time: 25.8838s\n",
      "\titers: 600, epoch: 15 | loss: 0.1230339\n",
      "\tspeed: 0.0434s/iter; left time: 21.6806s\n",
      "\titers: 700, epoch: 15 | loss: 0.1622226\n",
      "\tspeed: 0.0435s/iter; left time: 17.3889s\n",
      "\titers: 800, epoch: 15 | loss: 0.1176315\n",
      "\tspeed: 0.0437s/iter; left time: 13.0979s\n",
      "\titers: 900, epoch: 15 | loss: 0.1428157\n",
      "\tspeed: 0.0435s/iter; left time: 8.7018s\n",
      "\titers: 1000, epoch: 15 | loss: 0.1588525\n",
      "\tspeed: 0.0436s/iter; left time: 4.3585s\n",
      "Epoch: 15 cost time: 47.72319149971008\n",
      "Backbone Epoch: 10, Steps: 1099 | Train Loss: 0.1296220 Vali Loss: 0.1106595 Test Loss: 0.1306458\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "Updating learning rate to 1.953125e-06\n",
      "Average time per pretrain epoch: 34.34614s\n",
      "Average time per joint train epoch: 63.42058s\n",
      "test 5165\n",
      "mse:0.13067123293876648, mae:0.22774435579776764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.13067123, 0.22774436)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting = 'test'\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2689\n",
      "loading model\n",
      "mse:0.4212721288204193, mae:0.4345196485519409\n"
     ]
    }
   ],
   "source": [
    "test(setting, test=1, stationarity_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('result.csv')\n",
    "result_list = []\n",
    "for i in range(len(df)):\n",
    "    settings = df.Setting[i].split('_')\n",
    "    dataset = settings[1]\n",
    "    model_name = settings[4]\n",
    "    seq_len = settings[7][2:]\n",
    "    pred_len = settings[9][2:]\n",
    "    trend_norm = settings[-4][2:]\n",
    "    station_lr = settings[-3][3:]\n",
    "    use_mlp = settings[-2][2:]\n",
    "    mse = df.MSE[i]\n",
    "    mae = df.MAE[i]\n",
    "    \n",
    "    result_list.append([dataset, model_name, seq_len, pred_len, trend_norm, station_lr, use_mlp, mse, mae])\n",
    "df_result = pd.DataFrame(result_list, columns=['Dataset', 'Model', 'Seq_Len', 'Pred_Len', 'Trend_Norm', 'Station_LR', 'Use_MLP', 'MSE', 'MAE'])\n",
    "df_result.to_csv('summary_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
