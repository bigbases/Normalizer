{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from types import SimpleNamespace\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "\n",
    "from utils.metrics import metric\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from data_provider.data_loader import Dataset_Custom, Dataset_ETT_hour, Dataset_ETT_minute\n",
    "from models import DLinear, iTransformer, Autoformer, FEDformer, Linear, LSTM\n",
    "from models.trend import exp_1, exp_2, exp_3, exp_4\n",
    "from normalizers import SAN, DDN, TP, RevIN, NoNorm\n",
    "from layers.Autoformer_EncDec import series_decomp\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    task_name = \"long_term_forecast\",\n",
    "    model_name = \"exp_4\", \n",
    "    \n",
    "    # Task\n",
    "    seq_len=336,\n",
    "    label_len=0,\n",
    "    pred_len=336,\n",
    "    enc_in=7,\n",
    "    dec_in=7,\n",
    "    c_out=7,\n",
    "    features='M',\n",
    "    freq='h',\n",
    "    \n",
    "    # Data\n",
    "    root_path='datasets/ETT-small',\n",
    "    data='ETTh1',\n",
    "    data_path='ETTh1.csv',\n",
    "    target='OT',\n",
    "    batch_size=32,\n",
    "    \n",
    "    # Basic Model Settings\n",
    "    checkpoints='./trend_exp/checkpoints/',\n",
    "    dropout=0.1,\n",
    "    embed='timeF',\n",
    "    output_attention=False,\n",
    "    activation='gelu',\n",
    "    moving_avg=25,\n",
    "    num_kernels=6,\n",
    "    individual=False,\n",
    "    learning_rate=0.001,\n",
    "    n_heads=8,\n",
    "    patience=3,\n",
    "    train_epochs=10,\n",
    "    use_amp=False,\n",
    "    lradj='type1',\n",
    "    # d_model=512,\n",
    "    # d_ff=2048,\n",
    "    \n",
    "    # iTransformer\n",
    "    # d_model=128,\n",
    "    # d_ff=128,\n",
    "    # factor=1,\n",
    "    # e_layers=2,\n",
    "    # class_strategy='projection',\n",
    "    \n",
    "    # Autoformer\n",
    "    # factor=3,\n",
    "    # e_layers=2,\n",
    "    # d_layers=1,\n",
    "    \n",
    "    # FEDformer\n",
    "    # factor=3,\n",
    "    # e_layers=2,\n",
    "    # d_layers=1,\n",
    "    \n",
    "    # Normalizer\n",
    "    use_norm='none',\n",
    "    station_type='adaptive',\n",
    "    affine=True,\n",
    "    period_len=24,\n",
    "    station_lr=0.0005,\n",
    "    pre_epoch=5,\n",
    "    twice_epoch=0,\n",
    "    j=1,\n",
    "    learnable=False,\n",
    "    wavelet='coif3',\n",
    "    dr=0.05,\n",
    "    kernel_len=7,\n",
    "    hkernel_len=5,\n",
    "    pd_ff=128,\n",
    "    pd_model=128,\n",
    "    pe_layers=0,\n",
    "    kernel_size=25,\n",
    "    s_norm=True,\n",
    "    t_norm=True,\n",
    ")\n",
    "model_dict={\n",
    "    'DLinear': DLinear,\n",
    "    'iTransformer': iTransformer,\n",
    "    'Autoformer': Autoformer,\n",
    "    'FEDformer': FEDformer,\n",
    "    'Linear': Linear,\n",
    "    'LSTM': LSTM,\n",
    "    'exp_1': exp_1,\n",
    "    'exp_2': exp_2,\n",
    "    'exp_3': exp_3,\n",
    "    'exp_4': exp_4,\n",
    "}\n",
    "linear_models = ['DLinear', 'Linear', 'LSTM', 'exp_1', 'exp_2', 'exp_3', 'exp_4']\n",
    "norm_dict = {\n",
    "    'none': NoNorm,\n",
    "    'revin': RevIN,\n",
    "    'san': SAN,\n",
    "    'ddn': DDN,\n",
    "    'tp': TP,\n",
    "}\n",
    "decomp = series_decomp(25).cuda()\n",
    "\n",
    "# SAN\n",
    "def san_loss(y, statistics_pred):\n",
    "    bs, len, dim = y.shape\n",
    "    y = y.reshape(bs, -1, configs.period_len, dim)\n",
    "    mean = torch.mean(y, dim=2)\n",
    "    std = torch.std(y, dim=2)\n",
    "    station_ture = torch.cat([mean, std], dim=-1)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# DDN\n",
    "def ddn_loss(y, statistics_pred):\n",
    "    _, (mean, std) = norm.norm(y.transpose(-1, -2), False)\n",
    "    station_ture = torch.cat([mean, std], dim=1).transpose(-1, -2)\n",
    "    loss = criterion(statistics_pred, station_ture)\n",
    "    return loss\n",
    "\n",
    "# TREAD\n",
    "def tread_loss(y, statistics_pred):\n",
    "    trend_pred = statistics_pred[-1]\n",
    "    _, trend_true = decomp(y)\n",
    "    loss = criterion(trend_pred, trend_true)\n",
    "    return loss\n",
    "\n",
    "station_loss_dict = {\n",
    "    'none': None,\n",
    "    'revin': None,\n",
    "    'san': san_loss,\n",
    "    'ddn': ddn_loss,\n",
    "    'tp': tread_loss,\n",
    "}\n",
    "station_loss = station_loss_dict[configs.use_norm]\n",
    "\n",
    "# [pre train, pre epoch, joint train, join epoch]\n",
    "station_setting_dict = {\n",
    "    'none': [0, 0, 0, 0],\n",
    "    'revin': [0, 0, 0, 0],\n",
    "    'san': [1, configs.pre_epoch, 0, 0],\n",
    "    # 'san': [0, 0, 1, configs.twice_epoch],\n",
    "    'ddn': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "    'tp': [1, configs.pre_epoch, 1, configs.twice_epoch],\n",
    "}\n",
    "station_setting = station_setting_dict[configs.use_norm]\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "def criterion_2(true, pred, seasonal_pred, trend_pred):\n",
    "    seasonal_true, trend_true = decomp(true)\n",
    "    # loss1 = criterion(pred, true)\n",
    "    loss2 = criterion(seasonal_pred, seasonal_true.cuda())\n",
    "    loss3 = criterion(trend_pred, trend_true.cuda())\n",
    "    return loss2 + loss3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_dict[configs.model_name].Model(configs).float().cuda()\n",
    "norm = norm_dict[configs.use_norm].Model(configs).float().cuda()\n",
    "model_optim = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "norm_optim = optim.Adam(norm.parameters(), lr=configs.station_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion, epoch):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # Seasonal or Trend\n",
    "            seasonal_x, trend_x = decomp(batch_x)\n",
    "            seasonal_y, trend_y = decomp(batch_y)\n",
    "            # batch_x = trend_x\n",
    "            # batch_y = trend_y\n",
    "            # batch_x = seasonal_x\n",
    "            # batch_y = seasonal_y\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "\n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float()\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs, seasonal_pred, trend_pred = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion_2(pred, true, seasonal_pred, trend_pred)\n",
    "\n",
    "            total_loss.append(loss.cpu().item())\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    norm.train()\n",
    "    return total_loss\n",
    "\n",
    "def _get_data(flag):\n",
    "    data_set, data_loader = data_provider(configs, flag)\n",
    "    return data_set, data_loader\n",
    "\n",
    "def train(setting):\n",
    "    train_data, train_loader = _get_data(flag='train')\n",
    "    vali_data, vali_loader = _get_data(flag='val')\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    path = os.path.join(configs.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    path_station = './station/' + '{}_s{}_p{}'.format(configs.use_norm, configs.data,\n",
    "                                                        configs.seq_len, configs.pred_len)\n",
    "    if not os.path.exists(path_station):\n",
    "        os.makedirs(path_station)\n",
    "\n",
    "    time_now = time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "    early_stopping_station_model = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "\n",
    "    if configs.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(configs.train_epochs + station_setting[1]):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        \n",
    "        # Load best station model after pretraining\n",
    "        if station_setting[0] > 0 and epoch == station_setting[1]:\n",
    "            best_model_path = path_station + '/' + 'checkpoint.pth'\n",
    "            norm.load_state_dict(torch.load(best_model_path))\n",
    "            print('loading pretrained adaptive station model')\n",
    "        \n",
    "        # Add station parameters to model optim after pretraining and delay epochs for joint training\n",
    "        if station_setting[2] > 0 and station_setting[3] == epoch - station_setting[1]:\n",
    "            lr = model_optim.param_groups[0]['lr']\n",
    "            model_optim.add_param_group({'params': norm.parameters(), 'lr': lr})\n",
    "        \n",
    "        model.train()\n",
    "        norm.train()\n",
    "        epoch_time = time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            # Seasonal or Trend\n",
    "            seasonal_x, trend_x = decomp(batch_x)\n",
    "            seasonal_y, trend_y = decomp(batch_y)\n",
    "            # batch_x = trend_x\n",
    "            # batch_y = trend_y\n",
    "            # batch_x = seasonal_x\n",
    "            # batch_y = seasonal_y\n",
    "            \n",
    "            # normalize\n",
    "            if configs.use_norm == 'ddn':\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x, p_value=False)\n",
    "                else:\n",
    "                    batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            \n",
    "            # station pretrain\n",
    "            if epoch + 1 <= station_setting[1]:\n",
    "                f_dim = -1 if configs.features == 'MS' else 0\n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                if configs.features == 'MS':\n",
    "                    statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                loss = station_loss(batch_y, statistics_pred)\n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            # model train\n",
    "            else:\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                # decoder x\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "                dec_label = batch_x[:, -configs.label_len:, :]\n",
    "                dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if configs.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if configs.model_name in linear_models:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x)\n",
    "                        else:\n",
    "                            if configs.output_attention:\n",
    "                                outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                        f_dim = -1 if configs.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                        loss = criterion_2(outputs, batch_y, seasonal_pred, trend_pred)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs, seasonal_pred, trend_pred = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                    f_dim = -1 if configs.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "                    if configs.features == 'MS':\n",
    "                        statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                        \n",
    "                # de-normalize\n",
    "                outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "                \n",
    "                batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "                loss = criterion_2(outputs, batch_y, seasonal_pred, trend_pred)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time() - time_now) / iter_count\n",
    "                left_time = speed * (\n",
    "                        (configs.train_epochs + station_setting[1] - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time()\n",
    "            if configs.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                # two-stage training schema\n",
    "                if epoch + 1 <= station_setting[1]:\n",
    "                    norm_optim.step()\n",
    "                else:\n",
    "                    model_optim.step()\n",
    "                model_optim.zero_grad()\n",
    "                norm_optim.zero_grad()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion_2, epoch)\n",
    "        test_loss = vali(test_data, test_loader, criterion_2, epoch)\n",
    "\n",
    "        if epoch + 1 <= station_setting[1]:\n",
    "            print(\n",
    "                \"Station Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping_station_model(vali_loss, norm, path_station)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1, configs, configs.station_lr)\n",
    "        else:\n",
    "            print(\n",
    "                \"Backbone Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1 - station_setting[1], train_steps, train_loss, vali_loss, test_loss))\n",
    "            # if: joint training, else: only model training\n",
    "            if station_setting[2] > 0 and station_setting[3] <= epoch - station_setting[1]:\n",
    "                early_stopping(vali_loss, model, path, norm, path_station)\n",
    "            else:\n",
    "                early_stopping(vali_loss, model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(model_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.learning_rate)\n",
    "            adjust_learning_rate(norm_optim, epoch + 1 - station_setting[1], configs,\n",
    "                                    configs.station_lr)\n",
    "\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if station_setting[2] > 0:\n",
    "        norm.load_state_dict(torch.load(path_station + '/' + 'checkpoint.pth'))\n",
    "    return model\n",
    "\n",
    "def check_stationarity(batch_x, test='adf'):\n",
    "    ts = batch_x[0, :, -1]\n",
    "    if test == 'adf':\n",
    "        stat, pvalue, *_ = adfuller(ts)\n",
    "        # 낮은 p-value일수록 정상성 ↑ → 점수는 1 - pvalue\n",
    "        score = 1 - min(pvalue, 1.0)\n",
    "    elif test == 'kpss':\n",
    "        stat, pvalue, *_ = kpss(ts, nlags=\"auto\")\n",
    "        # 높은 p-value일수록 정상성 ↑\n",
    "        score = min(pvalue, 1.0)\n",
    "    return score\n",
    "\n",
    "def test(setting, test=0):\n",
    "    test_data, test_loader = _get_data(flag='test')\n",
    "\n",
    "    if test:\n",
    "        print('loading model')\n",
    "        model.load_state_dict(torch.load(os.path.join('./trend_exp/checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    inputx = []\n",
    "    folder_path = './trend_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    model.eval()\n",
    "    norm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            input_x = batch_x\n",
    "            \n",
    "            # Seasonal or Trend\n",
    "            # batch_x = trend_x\n",
    "            # batch_y = trend_y\n",
    "            # batch_x = seasonal_x\n",
    "            # batch_y = seasonal_y\n",
    "\n",
    "            # normalize\n",
    "            input_noNorm = batch_x\n",
    "            if configs.use_norm == 'ddn':\n",
    "                batch_x, statistics_pred, statistics_seq = norm.normalize(batch_x)\n",
    "            else:\n",
    "                batch_x, statistics_pred = norm.normalize(batch_x)\n",
    "            input_withNorm = batch_x\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            # decoder x\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "            dec_label = batch_x[:, -configs.label_len:, :]\n",
    "            dec_inp = torch.cat([dec_label, dec_inp], dim=1).float().to(device)\n",
    "            # encoder - decoder\n",
    "            if configs.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if configs.model_name in linear_models:\n",
    "                        outputs, seasonal_pred, trend_pred = model(batch_x)\n",
    "                    else:\n",
    "                        if configs.output_attention:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            else:\n",
    "                if configs.model_name in linear_models:\n",
    "                    outputs, seasonal_pred, trend_pred = model(batch_x)\n",
    "                else:\n",
    "                    if configs.output_attention:\n",
    "                        outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs, seasonal_pred, trend_pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "            f_dim = -1 if configs.features == 'MS' else 0\n",
    "            outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "            if configs.features == 'MS':\n",
    "                statistics_pred = statistics_pred[:, :, [configs.enc_in - 1, -1]]\n",
    "                \n",
    "            # de-normalize\n",
    "            output_noDenorm = outputs\n",
    "            outputs = norm.de_normalize(outputs, statistics_pred)\n",
    "            output_withDenorm = outputs\n",
    "            \n",
    "            batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "            pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "            true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "            inputx.append(batch_x.detach().cpu().numpy())\n",
    "            if i % 10 == 0:\n",
    "                x = input_x.detach().cpu().numpy()\n",
    "                gt = np.concatenate((x[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                pd = np.concatenate((x[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "            \n",
    "            # visualize input and output with/without normalization/denormalization\n",
    "            if i == 0:\n",
    "                i_n = input_noNorm.detach().cpu().numpy()\n",
    "                i_w = input_withNorm.detach().cpu().numpy()\n",
    "                o_n = output_noDenorm.detach().cpu().numpy()\n",
    "                o_w = output_withDenorm.detach().cpu().numpy()\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                # plt.subplot(3, 1, 1)\n",
    "                # plt.plot(i_w[0, :, -1], label='Normalized', color='darkorange')\n",
    "                # plt.plot(i_n[0, :, -1], label='Raw')\n",
    "                # plt.legend()\n",
    "                # plt.grid()\n",
    "                \n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(i_n[0, :, -1], label='Raw')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(i_w[0, :, -1], label='Normalized', color='darkorange')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_input.pdf'))\n",
    "                \n",
    "                plt.close('all')\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(true[0, :, -1], label='Ground Truth', color='black', linewidth=2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(0, label='Before Norm(Model Output)')\n",
    "                # legend outside of plot\n",
    "                plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3)\n",
    "                \n",
    "                plt.grid()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(o_w[0, :, -1], label='After Norm(Final Output)')\n",
    "                plt.plot(o_n[0, :, -1], label='Before Norm(Model Output)')\n",
    "                # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(folder_path, 'normalized_output.pdf'))\n",
    "\n",
    "    # if configs.test_flop:\n",
    "    #     test_params_flop((batch_x.shape[1], batch_x.shape[2]))\n",
    "    #     exit()\n",
    "    preds = np.array(preds, dtype=object)\n",
    "    trues = np.array(trues, dtype=object)\n",
    "    # inputx = np.array(inputx)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    # inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "    # result save\n",
    "    folder_path = './trend_exp/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "    print('mse:{}, mae:{}'.format(mse, mae))\n",
    "    f = open(\"result.txt\", 'a')\n",
    "    f.write(setting + \"  \\n\")\n",
    "    f.write('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n",
    "    # np.save(folder_path + 'pred.npy', preds)\n",
    "    # np.save(folder_path + 'true.npy', trues)\n",
    "    # np.save(folder_path + 'x.npy', inputx)\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7969\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.0006470\n",
      "\tspeed: 0.0387s/iter; left time: 92.4931s\n",
      "\titers: 200, epoch: 1 | loss: 0.0002946\n",
      "\tspeed: 0.0364s/iter; left time: 83.3421s\n",
      "Epoch: 1 cost time: 9.351967811584473\n",
      "Backbone Epoch: 1, Steps: 249 | Train Loss: 0.0010211 Vali Loss: 0.0002904 Test Loss: 0.0002885\n",
      "Validation loss decreased (inf --> 0.000290).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.0001655\n",
      "\tspeed: 0.0801s/iter; left time: 171.6096s\n",
      "\titers: 200, epoch: 2 | loss: 0.0001339\n",
      "\tspeed: 0.0362s/iter; left time: 73.9659s\n",
      "Epoch: 2 cost time: 9.102003574371338\n",
      "Backbone Epoch: 2, Steps: 249 | Train Loss: 0.0001668 Vali Loss: 0.0002148 Test Loss: 0.0001962\n",
      "Validation loss decreased (0.000290 --> 0.000215).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0000967\n",
      "\tspeed: 0.0781s/iter; left time: 147.8137s\n",
      "\titers: 200, epoch: 3 | loss: 0.0000947\n",
      "\tspeed: 0.0358s/iter; left time: 64.1004s\n",
      "Epoch: 3 cost time: 8.936881065368652\n",
      "Backbone Epoch: 3, Steps: 249 | Train Loss: 0.0001001 Vali Loss: 0.0001130 Test Loss: 0.0001096\n",
      "Validation loss decreased (0.000215 --> 0.000113).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0000839\n",
      "\tspeed: 0.0781s/iter; left time: 128.4255s\n",
      "\titers: 200, epoch: 4 | loss: 0.0000782\n",
      "\tspeed: 0.0324s/iter; left time: 50.0678s\n",
      "Epoch: 4 cost time: 8.575211524963379\n",
      "Backbone Epoch: 4, Steps: 249 | Train Loss: 0.0000815 Vali Loss: 0.0001001 Test Loss: 0.0000964\n",
      "Validation loss decreased (0.000113 --> 0.000100).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0000770\n",
      "\tspeed: 0.0774s/iter; left time: 108.0312s\n",
      "\titers: 200, epoch: 5 | loss: 0.0000748\n",
      "\tspeed: 0.0359s/iter; left time: 46.4284s\n",
      "Epoch: 5 cost time: 8.881272554397583\n",
      "Backbone Epoch: 5, Steps: 249 | Train Loss: 0.0000742 Vali Loss: 0.0000937 Test Loss: 0.0000899\n",
      "Validation loss decreased (0.000100 --> 0.000094).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0000709\n",
      "\tspeed: 0.0774s/iter; left time: 88.7263s\n",
      "\titers: 200, epoch: 6 | loss: 0.0000678\n",
      "\tspeed: 0.0359s/iter; left time: 37.5992s\n",
      "Epoch: 6 cost time: 8.91569209098816\n",
      "Backbone Epoch: 6, Steps: 249 | Train Loss: 0.0000704 Vali Loss: 0.0000904 Test Loss: 0.0000865\n",
      "Validation loss decreased (0.000094 --> 0.000090).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0000646\n",
      "\tspeed: 0.0747s/iter; left time: 66.9870s\n",
      "\titers: 200, epoch: 7 | loss: 0.0000691\n",
      "\tspeed: 0.0318s/iter; left time: 25.3688s\n",
      "Epoch: 7 cost time: 8.180092811584473\n",
      "Backbone Epoch: 7, Steps: 249 | Train Loss: 0.0000684 Vali Loss: 0.0000886 Test Loss: 0.0000847\n",
      "Validation loss decreased (0.000090 --> 0.000089).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0000665\n",
      "\tspeed: 0.0779s/iter; left time: 50.4688s\n",
      "\titers: 200, epoch: 8 | loss: 0.0000698\n",
      "\tspeed: 0.0341s/iter; left time: 18.6958s\n",
      "Epoch: 8 cost time: 8.733292818069458\n",
      "Backbone Epoch: 8, Steps: 249 | Train Loss: 0.0000672 Vali Loss: 0.0000876 Test Loss: 0.0000836\n",
      "Validation loss decreased (0.000089 --> 0.000088).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0000654\n",
      "\tspeed: 0.0777s/iter; left time: 31.0174s\n",
      "\titers: 200, epoch: 9 | loss: 0.0000663\n",
      "\tspeed: 0.0358s/iter; left time: 10.7115s\n",
      "Epoch: 9 cost time: 8.91344404220581\n",
      "Backbone Epoch: 9, Steps: 249 | Train Loss: 0.0000666 Vali Loss: 0.0000870 Test Loss: 0.0000830\n",
      "Validation loss decreased (0.000088 --> 0.000087).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.0000693\n",
      "\tspeed: 0.0783s/iter; left time: 11.7444s\n",
      "\titers: 200, epoch: 10 | loss: 0.0000629\n",
      "\tspeed: 0.0359s/iter; left time: 1.7967s\n",
      "Epoch: 10 cost time: 8.93907117843628\n",
      "Backbone Epoch: 10, Steps: 249 | Train Loss: 0.0000662 Vali Loss: 0.0000867 Test Loss: 0.0000827\n",
      "Validation loss decreased (0.000087 --> 0.000087).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "Updating learning rate to 9.765625e-07\n",
      "test 2545\n",
      "mse:0.9995078444480896, mae:0.6802395582199097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99950784, 0.68023956)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting = 'ETTh1_s336_p336_exp_4_none'\n",
    "train(setting)\n",
    "test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2689\n",
      "loading model\n",
      "mse:0.4212721288204193, mae:0.4345196485519409\n"
     ]
    }
   ],
   "source": [
    "test(setting, test=1, stationarity_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.plot(raw, label='Raw', color='black')\n",
    "# plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.plot(0, color='black')\n",
    "plt.plot(revin, label='RevIN')\n",
    "plt.legend(loc='upper right')\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.plot(0, color='black')\n",
    "plt.plot(0)\n",
    "plt.plot(san, label='SAN')\n",
    "plt.legend(loc='upper right')\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.plot(0, color='black')\n",
    "plt.plot(0)\n",
    "plt.plot(0)\n",
    "plt.plot(ddn, label='DDN')\n",
    "plt.legend(loc='upper right')\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.plot(0, color='black')\n",
    "plt.plot(0)\n",
    "plt.plot(0)\n",
    "plt.plot(0)\n",
    "plt.plot(tp, label='TREAD')\n",
    "plt.yticks([])\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"compare.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
